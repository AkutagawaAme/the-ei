{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cfc9dae1-fd68-4ad7-9292-0f8b686540e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smote_variants as sv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from collections import Counter\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24161249-e00d-4970-a4fe-b12ad12571a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FeatureSelection] 选择 top-8 特征： ['gc_13', 'x_c', 'gc_18', 'gc_21', 'z_21', 'age', 'ESI', 'GC']\n",
      "[FeatureSelection] 特征重要性（前6）：\n",
      " gc_13    0.127294\n",
      "x_c      0.117290\n",
      "gc_18    0.112395\n",
      "gc_21    0.097027\n",
      "z_21     0.077626\n",
      "age      0.065048\n",
      "dtype: float64\n",
      "=== 原始标签组合分布 ===\n",
      "_label_tuple\n",
      "(0, 0, 0)    342\n",
      "(0, 1, 0)     21\n",
      "(1, 1, 0)      9\n",
      "(1, 0, 0)      8\n",
      "(0, 0, 1)      5\n",
      "(1, 0, 1)      1\n",
      "(0, 1, 1)      1\n",
      "Name: count, dtype: int64\n",
      "[MLSMOTE] 使用目标样本数 target_count = 21（针对异常组合）\n",
      "[DEBUG MLSMOTE] ref_idx=1, neigh_idx=3, ref_label=[1 1 0], neigh_label=[1 1 0], new_label=[1 1 0]\n",
      "[DEBUG MLSMOTE] ref_idx=3, neigh_idx=7, ref_label=[1 1 0], neigh_label=[1 1 0], new_label=[1 1 0]\n",
      "[DEBUG MLSMOTE] ref_idx=1, neigh_idx=6, ref_label=[1 1 0], neigh_label=[1 1 0], new_label=[1 1 0]\n",
      "[MLSMOTE] 组合 (1, 1, 0) 当前 9 -> 生成 12\n",
      "[DEBUG MLSMOTE] ref_idx=4, neigh_idx=1, ref_label=[1 0 0], neigh_label=[1 0 0], new_label=[1 0 0]\n",
      "[DEBUG MLSMOTE] ref_idx=1, neigh_idx=2, ref_label=[1 0 0], neigh_label=[1 0 0], new_label=[1 0 0]\n",
      "[DEBUG MLSMOTE] ref_idx=4, neigh_idx=3, ref_label=[1 0 0], neigh_label=[1 0 0], new_label=[1 0 0]\n",
      "[MLSMOTE] 组合 (1, 0, 0) 当前 8 -> 生成 13\n",
      "[DEBUG MLSMOTE] ref_idx=0, neigh_idx=1, ref_label=[0 0 1], neigh_label=[0 0 1], new_label=[0 0 1]\n",
      "[DEBUG MLSMOTE] ref_idx=0, neigh_idx=2, ref_label=[0 0 1], neigh_label=[0 0 1], new_label=[0 0 1]\n",
      "[DEBUG MLSMOTE] ref_idx=4, neigh_idx=1, ref_label=[0 0 1], neigh_label=[0 0 1], new_label=[0 0 1]\n",
      "[MLSMOTE] 组合 (0, 0, 1) 当前 5 -> 生成 16\n",
      "[MLSMOTE] 单样本组合 (1, 0, 1) 当前 1 -> 生成 4\n",
      "[MLSMOTE] 单样本组合 (0, 1, 1) 当前 1 -> 生成 4\n",
      "\n",
      "=== 平衡后标签组合分布 ===\n",
      "(0, 0, 0)    342\n",
      "(1, 1, 0)     21\n",
      "(0, 1, 0)     21\n",
      "(0, 0, 1)     21\n",
      "(1, 0, 0)     21\n",
      "(1, 0, 1)      5\n",
      "(0, 1, 1)      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== 平衡后单标签数量 ===\n",
      "AOC_T13    47\n",
      "AOC_T18    47\n",
      "AOC_T21    31\n",
      "dtype: int64\n",
      "\n",
      "健康样本数: 342\n",
      "异常样本数: 94\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "def nearest_neighbour(X, n_neighbors=5):\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else np.asarray(X)\n",
    "    n = X_np.shape[0]\n",
    "    if n == 0:\n",
    "        return np.empty((0, 0), dtype=int)\n",
    "    k = min(n_neighbors + 1, n)  # +1 to include self\n",
    "    nbs = NearestNeighbors(n_neighbors=k, metric='euclidean', algorithm='kd_tree').fit(X_np)\n",
    "    _, indices = nbs.kneighbors(X_np)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def jaccard_similarity(y1, y2):\n",
    "    intersection = np.sum(np.logical_and(y1, y2))\n",
    "    union = np.sum(np.logical_or(y1, y2))\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def MLSMOTE_multilabel(X, y, n_sample, n_neighbors=5, add_noise=True):\n",
    "    \"\"\"\n",
    "    Standard MLSMOTE on a subset X,y (len>=2). Returns X_new, y_new (DataFrames).\n",
    "    Label strategy: union(ref, neighbour).\n",
    "    \"\"\"\n",
    "    if len(X) < 2:\n",
    "        # Should not call this for singletons; handle outside\n",
    "        return pd.DataFrame(columns=X.columns), pd.DataFrame(columns=y.columns)\n",
    "\n",
    "    indices2 = nearest_neighbour(X, n_neighbors=n_neighbors)\n",
    "    n = len(indices2)\n",
    "    new_X = np.zeros((n_sample, X.shape[1]))\n",
    "    new_y = np.zeros((n_sample, y.shape[1]))\n",
    "\n",
    "    global_range = (X.max() - X.min()).replace(0, 1e-6)\n",
    "\n",
    "    for i in range(n_sample):\n",
    "        reference = random.randint(0, n - 1)\n",
    "        neigh_indices = indices2[reference, :].tolist()\n",
    "        neigh_indices = [idx for idx in neigh_indices if idx != reference]\n",
    "\n",
    "        if not neigh_indices:\n",
    "            candidates = list(range(n))\n",
    "            candidates.remove(reference)\n",
    "            neighbour = random.choice(candidates)\n",
    "        else:\n",
    "            sims = [jaccard_similarity(y.iloc[reference].values, y.iloc[nb].values) for nb in neigh_indices]\n",
    "            max_sim = max(sims)\n",
    "            best_idxs = [neigh_indices[idx] for idx, s in enumerate(sims) if s == max_sim]\n",
    "            neighbour = random.choice(best_idxs)\n",
    "\n",
    "        ratio = random.random()\n",
    "        gap = X.iloc[neighbour, :].values - X.iloc[reference, :].values\n",
    "\n",
    "        if add_noise:\n",
    "            local_scale = np.abs(X.iloc[reference, :].values - X.iloc[neighbour, :].values)\n",
    "            std = 0.01 * np.where(local_scale == 0, global_range.values * 0.01, local_scale)\n",
    "            perturb = np.random.normal(loc=0.0, scale=std, size=X.shape[1])\n",
    "        else:\n",
    "            perturb = np.zeros(X.shape[1])\n",
    "\n",
    "        new_x = X.iloc[reference, :].values + ratio * gap + perturb\n",
    "        # clip to original feature range to avoid极端外推\n",
    "        new_x = np.minimum(np.maximum(new_x, X.min().values), X.max().values)\n",
    "\n",
    "        new_X[i, :] = new_x\n",
    "        y_ref = y.iloc[reference].values.astype(int)\n",
    "        y_nei = y.iloc[neighbour].values.astype(int)\n",
    "        new_label = np.logical_or(y_ref, y_nei).astype(int)\n",
    "        new_y[i, :] = new_label\n",
    "\n",
    "        if i < 3:\n",
    "            print(f\"[DEBUG MLSMOTE] ref_idx={reference}, neigh_idx={neighbour}, ref_label={y_ref}, neigh_label={y_nei}, new_label={new_label}\")\n",
    "\n",
    "    new_X_df = pd.DataFrame(new_X, columns=X.columns)\n",
    "    new_y_df = pd.DataFrame(new_y.astype(int), columns=y.columns)\n",
    "    return new_X_df, new_y_df\n",
    "\n",
    "\n",
    "def generate_from_single_sample(single_row, other_df, n_generate, feature_cols, label_cols, n_neighbors=5, add_noise=True):\n",
    "    \"\"\"\n",
    "    为单样本组合生成 n_generate 个合成样本：\n",
    "    - single_row: DataFrame 单行（含 feature & label）\n",
    "    - other_df: DataFrame 包含所有其他异常样本（含 feature & label）\n",
    "    策略：每次选一个最相似的其他异常样本（先按标签 Jaccard，再按特征距离）做插值。\n",
    "    \"\"\"\n",
    "    X_ref = single_row[feature_cols].iloc[0].values.astype(float)\n",
    "    y_ref = single_row[label_cols].iloc[0].values.astype(int)\n",
    "\n",
    "    if other_df.shape[0] == 0:\n",
    "        # 没有其他异常可用，退回：复制 + 小扰动\n",
    "        new_Xs, new_ys = [], []\n",
    "        global_scale = (single_row[feature_cols].max() - single_row[feature_cols].min())\n",
    "        for _ in range(n_generate):\n",
    "            perturb = np.random.normal(0, 0.01 * (global_scale.replace(0, 1e-6).values))\n",
    "            new_x = np.clip(X_ref + perturb, single_row[feature_cols].min().values, single_row[feature_cols].max().values)\n",
    "            new_Xs.append(new_x)\n",
    "            new_ys.append(y_ref)\n",
    "        return pd.DataFrame(new_Xs, columns=feature_cols), pd.DataFrame(np.array(new_ys).astype(int), columns=label_cols)\n",
    "\n",
    "    # 计算其他异常样本的标签相似度与特征距离\n",
    "    feats = other_df[feature_cols].values.astype(float)\n",
    "    labels = other_df[label_cols].values.astype(int)\n",
    "\n",
    "    new_Xs, new_ys = [], []\n",
    "    for _ in range(n_generate):\n",
    "        # jaccard scores\n",
    "        j_scores = np.array([jaccard_similarity(y_ref, lab) for lab in labels])\n",
    "        max_j = j_scores.max()\n",
    "        candidates_idx = np.where(j_scores == max_j)[0]\n",
    "        # 若多个相同 jaccard，按特征距离选最近\n",
    "        if len(candidates_idx) > 1:\n",
    "            dists = np.linalg.norm(feats[candidates_idx] - X_ref, axis=1)\n",
    "            choose_idx = candidates_idx[np.argmin(dists)]\n",
    "        else:\n",
    "            choose_idx = candidates_idx[0]\n",
    "\n",
    "        neighbor_feat = feats[choose_idx]\n",
    "        neighbor_label = labels[choose_idx]\n",
    "\n",
    "        ratio = random.random()\n",
    "        gap = neighbor_feat - X_ref\n",
    "\n",
    "        if add_noise:\n",
    "            local_scale = np.abs(X_ref - neighbor_feat)\n",
    "            std = 0.01 * np.where(local_scale == 0, np.std(feats, axis=0) * 0.01, local_scale)\n",
    "            perturb = np.random.normal(0, std)\n",
    "        else:\n",
    "            perturb = np.zeros_like(X_ref)\n",
    "\n",
    "        new_x = X_ref + ratio * gap + perturb\n",
    "        # clip 到全体其他异常的范围，防止极端\n",
    "        col_min = np.minimum(single_row[feature_cols].min().values, other_df[feature_cols].min().values)\n",
    "        col_max = np.maximum(single_row[feature_cols].max().values, other_df[feature_cols].max().values)\n",
    "        new_x = np.minimum(np.maximum(new_x, col_min), col_max)\n",
    "\n",
    "        new_label = np.logical_or(y_ref, neighbor_label).astype(int)\n",
    "\n",
    "        new_Xs.append(new_x)\n",
    "        new_ys.append(new_label)\n",
    "\n",
    "    return pd.DataFrame(new_Xs, columns=feature_cols), pd.DataFrame(np.array(new_ys).astype(int), columns=label_cols)\n",
    "\n",
    "\n",
    "# 需要的额外导入（放在文件头）\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "# ---------- 新增：多标签特征选择函数 ----------\n",
    "def feature_selection_multilabel(X, y, method='rf', n_features=None,\n",
    "                                 random_state=42, rf_n_estimators=200):\n",
    "    \"\"\"\n",
    "    多标签特征选择（基于 RandomForest + ClassifierChain）。\n",
    "    仅统计每个链中对原始特征部分的 feature_importances_。\n",
    "    \"\"\"\n",
    "    if method != 'rf':\n",
    "        raise ValueError(\"目前只支持 method='rf'\")\n",
    "\n",
    "    if X.shape[1] == 0:\n",
    "        return [], pd.Series(dtype=float)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=rf_n_estimators, random_state=random_state, n_jobs=-1)\n",
    "    chain = ClassifierChain(rf)\n",
    "    chain.fit(X, y)\n",
    "\n",
    "    n_features_total = X.shape[1]\n",
    "    all_importances = []\n",
    "\n",
    "    for est in chain.estimators_:\n",
    "        # 注意：后续链的输入是 [原始X, 已预测标签]，我们只取前 n_features_total 部分\n",
    "        imp = getattr(est, \"feature_importances_\", None)\n",
    "        if imp is not None:\n",
    "            if len(imp) >= n_features_total:\n",
    "                all_importances.append(imp[:n_features_total])\n",
    "            else:\n",
    "                # 理论上不会发生，但加保险\n",
    "                pad = np.zeros(n_features_total)\n",
    "                pad[:len(imp)] = imp\n",
    "                all_importances.append(pad)\n",
    "\n",
    "    if not all_importances:\n",
    "        print(\"[FeatureSelection] 所有子模型均无 feature_importances_，返回空列表。\")\n",
    "        return [], pd.Series(dtype=float)\n",
    "\n",
    "    mean_imps = np.mean(np.vstack(all_importances), axis=0)\n",
    "    feat_series = pd.Series(mean_imps, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    if n_features is None:\n",
    "        n_features = max(1, X.shape[1] // 2)\n",
    "    n_features = min(n_features, X.shape[1])\n",
    "    selected = feat_series.iloc[:n_features].index.tolist()\n",
    "\n",
    "    print(\"[FeatureSelection] 选择 top-%d 特征：\" % n_features, selected)\n",
    "    print(\"[FeatureSelection] 特征重要性（前6）：\\n\", feat_series.head(6))\n",
    "\n",
    "    return selected, feat_series\n",
    "\n",
    "\n",
    "def MLSMOTE_balance_by_labelset_with_singleton_support(X, y,\n",
    "                                                       min_samples=1,\n",
    "                                                       target_count=None,\n",
    "                                                       n_neighbors=5,\n",
    "                                                       add_noise=True,\n",
    "                                                       max_ratio=5,\n",
    "                                                       select_features=False,\n",
    "                                                       n_selected_features=None,\n",
    "                                                       select_method='rf',\n",
    "                                                       rf_n_estimators=200,\n",
    "                                                       selection_random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    与原函数行为一致，但新增 select_features 参数：\n",
    "    - select_features: 是否先做特征选择（仅在传入训练集时使用！）\n",
    "    - n_selected_features: 若不为 None，则选择固定数量特征；否则默认 top50%。\n",
    "    返回：X_res (DataFrame only selected features), y_res (DataFrame), selected_features (list or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # 复制输入以免修改外部变量\n",
    "    X = X.reset_index(drop=True).copy()\n",
    "    y = y.reset_index(drop=True).copy()\n",
    "\n",
    "    # 1) 特征选择（仅在用户开启时）\n",
    "    selected_features = None\n",
    "    if select_features:\n",
    "        selected_features, feat_importances = feature_selection_multilabel(\n",
    "            X, y,\n",
    "            method=select_method,\n",
    "            n_features=n_selected_features,\n",
    "            random_state=selection_random_state,\n",
    "            rf_n_estimators=rf_n_estimators\n",
    "        )\n",
    "        # 用选出的特征裁剪 X\n",
    "        if len(selected_features) == 0:\n",
    "            print(\"[FeatureSelection] 未选中任何特征，退回全特征。\")\n",
    "            selected_features = X.columns.tolist()\n",
    "        X = X[selected_features].reset_index(drop=True)\n",
    "\n",
    "    label_cols = y.columns.tolist()\n",
    "    feature_cols = X.columns.tolist()\n",
    "    df = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n",
    "    df['_label_tuple'] = df[label_cols].apply(lambda row: tuple(int(x) for x in row), axis=1)\n",
    "\n",
    "    labelset_counts = df['_label_tuple'].value_counts()\n",
    "    print(\"=== 原始标签组合分布 ===\")\n",
    "    print(labelset_counts)\n",
    "\n",
    "    healthy_label = tuple([0] * len(label_cols))\n",
    "    anomaly_counts = labelset_counts.drop(labels=[healthy_label], errors='ignore')\n",
    "    if anomaly_counts.shape[0] == 0:\n",
    "        print(\"[MLSMOTE] 未检测到异常组合，返回原始数据。\")\n",
    "        X_res = X.copy()\n",
    "        y_res = y.copy()\n",
    "        return X_res, y_res, selected_features\n",
    "\n",
    "    if target_count is None:\n",
    "        target_count = int(anomaly_counts.max())\n",
    "    print(f\"[MLSMOTE] 使用目标样本数 target_count = {target_count}（针对异常组合）\")\n",
    "    all_new = []\n",
    "\n",
    "    df_anomalies = df[df['_label_tuple'] != healthy_label].reset_index(drop=True)\n",
    "\n",
    "    for labelset, count in labelset_counts.items():\n",
    "        if labelset == healthy_label:\n",
    "            continue\n",
    "        count = int(count)\n",
    "        allowed_final = min(target_count, int(max(1, count * max_ratio)))\n",
    "        n_to_generate = max(0, allowed_final - count)\n",
    "\n",
    "        if n_to_generate <= 0:\n",
    "            continue\n",
    "\n",
    "        if count >= 2:\n",
    "            df_sub = df[df['_label_tuple'] == labelset].reset_index(drop=True)\n",
    "            X_sub = df_sub[feature_cols]\n",
    "            y_sub = df_sub[label_cols]\n",
    "            X_new, y_new = MLSMOTE_multilabel(X_sub, y_sub, n_to_generate, n_neighbors=n_neighbors, add_noise=add_noise)\n",
    "            if not X_new.empty:\n",
    "                df_new = pd.concat([X_new.reset_index(drop=True), y_new.reset_index(drop=True)], axis=1)\n",
    "                df_new['_label_tuple'] = [labelset] * len(df_new)\n",
    "                all_new.append(df_new)\n",
    "            print(f\"[MLSMOTE] 组合 {labelset} 当前 {count} -> 生成 {len(X_new)}\")\n",
    "        else:\n",
    "            df_sub = df[df['_label_tuple'] == labelset].reset_index(drop=True)\n",
    "            df_other = df_anomalies[df_anomalies['_label_tuple'] != labelset].reset_index(drop=True)\n",
    "            # 这里传入 feature_cols、label_cols（feature_cols 已根据选择裁剪）\n",
    "            X_new, y_new = generate_from_single_sample(df_sub, df_other, n_to_generate, feature_cols, label_cols, n_neighbors=n_neighbors, add_noise=add_noise)\n",
    "            if not X_new.empty:\n",
    "                df_new = pd.concat([X_new.reset_index(drop=True), y_new.reset_index(drop=True)], axis=1)\n",
    "                df_new['_label_tuple'] = [labelset] * len(df_new)\n",
    "                all_new.append(df_new)\n",
    "            print(f\"[MLSMOTE] 单样本组合 {labelset} 当前 {count} -> 生成 {len(X_new)}\")\n",
    "\n",
    "    if all_new:\n",
    "        df_new_all = pd.concat(all_new, axis=0).reset_index(drop=True)\n",
    "        df_combined = pd.concat([df, df_new_all], axis=0).reset_index(drop=True)\n",
    "    else:\n",
    "        df_combined = df.copy()\n",
    "\n",
    "    cols_keep = feature_cols + label_cols\n",
    "    df_combined = df_combined[cols_keep].drop_duplicates().reset_index(drop=True)\n",
    "    X_res = df_combined[feature_cols].reset_index(drop=True)\n",
    "    y_res = df_combined[label_cols].astype(int).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== 平衡后标签组合分布 ===\")\n",
    "    print(y_res.apply(lambda row: tuple(int(x) for x in row), axis=1).value_counts())\n",
    "\n",
    "    return X_res, y_res, selected_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_pw = pd.read_excel(\"girl.xlsx\")\n",
    "    feature_cols = ['age', 'pw_bmi', 'GC', 'z_13', 'z_18', 'z_21',\n",
    "                    'x_z', 'x_c', 'gc_13', 'gc_18', 'gc_21',\n",
    "                    'pregnancies_times', 'ORS_B', 'ESI']\n",
    "    label_cols = ['AOC_T13', 'AOC_T18', 'AOC_T21']\n",
    "\n",
    "    X = df_pw[feature_cols]\n",
    "    y = df_pw[label_cols]\n",
    "\n",
    "    train_xval, test_x, train_yval, test_y = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    train_x, X_val, train_y, y_val = train_test_split(train_xval, train_yval, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    train_x_res, train_y_res, selected_feats = MLSMOTE_balance_by_labelset_with_singleton_support(\n",
    "        train_x, train_y,\n",
    "        min_samples=1,\n",
    "        target_count=None,\n",
    "        n_neighbors=5,\n",
    "        add_noise=True,\n",
    "        max_ratio=5,\n",
    "        select_features=True,        # 开启特征选择\n",
    "        n_selected_features=8,       # 选择 top-8 特征（你可以根据验证调整）\n",
    "        select_method='rf',\n",
    "        rf_n_estimators=300,\n",
    "        selection_random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # 之后对训练数据标准化（仅对被选的特征）\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_x_res_scaled = scaler.fit_transform(train_x_res)         # 训练集 fit_transform\n",
    "    # 对验证/测试集做同样的特征裁剪和 transform\n",
    "    test_x_selected = test_x[selected_feats]                       # 同样只保留被选特征    \n",
    "    train_x_selected = train_x[selected_feats]                     # 原始训练集裁剪特征\n",
    "    train_x_selected = scaler.fit_transform(train_x_selected)\n",
    "    test_x_selected = scaler.transform(test_x_selected)\n",
    "\n",
    "    print(\"\\n=== 平衡后单标签数量 ===\")\n",
    "    print(train_y_res.sum())\n",
    "\n",
    "    print(\"\\n健康样本数:\", (train_y_res.sum(axis=1) == 0).sum())\n",
    "    print(\"异常样本数:\", (train_y_res.sum(axis=1) > 0).sum())\n",
    "\n",
    "    # 可视化\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    pd.DataFrame(train_y_res, columns=label_cols).sum(axis=0).plot(kind='bar', title='T13/T18/T21 标签数量分布')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eeb1a0-dd16-4bad-871f-432b2fb1625e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 运行：MLKNN (有 MLSMOTE) 网格搜索 ===\n",
      "[MLSMOTE] k=3, s=1, alpha=0.00 -> Hamming=0.0744, F1m=0.3415\n",
      "[MLSMOTE] k=3, s=1, alpha=0.30 -> Hamming=0.0882, F1m=0.3333\n",
      "[MLSMOTE] k=3, s=1, alpha=0.60 -> Hamming=0.1322, F1m=0.2500\n",
      "[MLSMOTE] k=3, s=1, alpha=0.90 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=1, alpha=1.00 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=2, alpha=0.00 -> Hamming=0.0744, F1m=0.3415\n",
      "[MLSMOTE] k=3, s=2, alpha=0.30 -> Hamming=0.0882, F1m=0.3333\n",
      "[MLSMOTE] k=3, s=2, alpha=0.60 -> Hamming=0.1295, F1m=0.2540\n",
      "[MLSMOTE] k=3, s=2, alpha=0.90 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=2, alpha=1.00 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=3, alpha=0.00 -> Hamming=0.0744, F1m=0.3415\n",
      "[MLSMOTE] k=3, s=3, alpha=0.30 -> Hamming=0.0882, F1m=0.3333\n",
      "[MLSMOTE] k=3, s=3, alpha=0.60 -> Hamming=0.1295, F1m=0.2540\n",
      "[MLSMOTE] k=3, s=3, alpha=0.90 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=3, alpha=1.00 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=4, alpha=0.00 -> Hamming=0.0744, F1m=0.3415\n",
      "[MLSMOTE] k=3, s=4, alpha=0.30 -> Hamming=0.0799, F1m=0.3556\n",
      "[MLSMOTE] k=3, s=4, alpha=0.60 -> Hamming=0.1295, F1m=0.2540\n",
      "[MLSMOTE] k=3, s=4, alpha=0.90 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=4, alpha=1.00 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=5, alpha=0.00 -> Hamming=0.0523, F1m=0.3448\n",
      "[MLSMOTE] k=3, s=5, alpha=0.30 -> Hamming=0.0799, F1m=0.3556\n",
      "[MLSMOTE] k=3, s=5, alpha=0.60 -> Hamming=0.1267, F1m=0.2581\n",
      "[MLSMOTE] k=3, s=5, alpha=0.90 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=3, s=5, alpha=1.00 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=4, s=1, alpha=0.00 -> Hamming=0.0579, F1m=0.4000\n",
      "[MLSMOTE] k=4, s=1, alpha=0.30 -> Hamming=0.1019, F1m=0.3019\n",
      "[MLSMOTE] k=4, s=1, alpha=0.60 -> Hamming=0.1074, F1m=0.2909\n",
      "[MLSMOTE] k=4, s=1, alpha=0.90 -> Hamming=0.1240, F1m=0.2623\n",
      "[MLSMOTE] k=4, s=1, alpha=1.00 -> Hamming=0.1377, F1m=0.2424\n",
      "[MLSMOTE] k=4, s=2, alpha=0.00 -> Hamming=0.0579, F1m=0.4000\n",
      "[MLSMOTE] k=4, s=2, alpha=0.30 -> Hamming=0.1019, F1m=0.3019\n",
      "[MLSMOTE] k=4, s=2, alpha=0.60 -> Hamming=0.1047, F1m=0.2963\n",
      "[MLSMOTE] k=4, s=2, alpha=0.90 -> Hamming=0.1185, F1m=0.2712\n",
      "[MLSMOTE] k=4, s=2, alpha=1.00 -> Hamming=0.1240, F1m=0.2623\n",
      "[MLSMOTE] k=4, s=3, alpha=0.00 -> Hamming=0.0579, F1m=0.4000\n",
      "[MLSMOTE] k=4, s=3, alpha=0.30 -> Hamming=0.1019, F1m=0.3019\n",
      "[MLSMOTE] k=4, s=3, alpha=0.60 -> Hamming=0.1047, F1m=0.2963\n",
      "[MLSMOTE] k=4, s=3, alpha=0.90 -> Hamming=0.1157, F1m=0.2759\n",
      "[MLSMOTE] k=4, s=3, alpha=1.00 -> Hamming=0.1240, F1m=0.2623\n",
      "[MLSMOTE] k=4, s=4, alpha=0.00 -> Hamming=0.0579, F1m=0.4000\n",
      "[MLSMOTE] k=4, s=4, alpha=0.30 -> Hamming=0.1019, F1m=0.2745\n",
      "[MLSMOTE] k=4, s=4, alpha=0.60 -> Hamming=0.1047, F1m=0.2963\n",
      "[MLSMOTE] k=4, s=4, alpha=0.90 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=4, s=4, alpha=1.00 -> Hamming=0.1185, F1m=0.2712\n",
      "[MLSMOTE] k=4, s=5, alpha=0.00 -> Hamming=0.0579, F1m=0.4000\n",
      "[MLSMOTE] k=4, s=5, alpha=0.30 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=4, s=5, alpha=0.60 -> Hamming=0.1019, F1m=0.3019\n",
      "[MLSMOTE] k=4, s=5, alpha=0.90 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=4, s=5, alpha=1.00 -> Hamming=0.1185, F1m=0.2712\n",
      "[MLSMOTE] k=5, s=1, alpha=0.00 -> Hamming=0.0634, F1m=0.3784\n",
      "[MLSMOTE] k=5, s=1, alpha=0.30 -> Hamming=0.0826, F1m=0.3182\n",
      "[MLSMOTE] k=5, s=1, alpha=0.60 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=1, alpha=0.90 -> Hamming=0.1157, F1m=0.2759\n",
      "[MLSMOTE] k=5, s=1, alpha=1.00 -> Hamming=0.1185, F1m=0.2712\n",
      "[MLSMOTE] k=5, s=2, alpha=0.00 -> Hamming=0.0634, F1m=0.3784\n",
      "[MLSMOTE] k=5, s=2, alpha=0.30 -> Hamming=0.0799, F1m=0.3256\n",
      "[MLSMOTE] k=5, s=2, alpha=0.60 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=2, alpha=0.90 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=2, alpha=1.00 -> Hamming=0.1129, F1m=0.2807\n",
      "[MLSMOTE] k=5, s=3, alpha=0.00 -> Hamming=0.0634, F1m=0.3784\n",
      "[MLSMOTE] k=5, s=3, alpha=0.30 -> Hamming=0.0799, F1m=0.3256\n",
      "[MLSMOTE] k=5, s=3, alpha=0.60 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=3, alpha=0.90 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=3, alpha=1.00 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=4, alpha=0.00 -> Hamming=0.0634, F1m=0.3429\n",
      "[MLSMOTE] k=5, s=4, alpha=0.30 -> Hamming=0.0799, F1m=0.3256\n",
      "[MLSMOTE] k=5, s=4, alpha=0.60 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=4, alpha=0.90 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=4, alpha=1.00 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=5, alpha=0.00 -> Hamming=0.0634, F1m=0.3429\n",
      "[MLSMOTE] k=5, s=5, alpha=0.30 -> Hamming=0.0716, F1m=0.3500\n",
      "[MLSMOTE] k=5, s=5, alpha=0.60 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=5, alpha=0.90 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=5, s=5, alpha=1.00 -> Hamming=0.1102, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=1, alpha=0.00 -> Hamming=0.0744, F1m=0.2286\n",
      "[MLSMOTE] k=7, s=1, alpha=0.30 -> Hamming=0.0964, F1m=0.2553\n",
      "[MLSMOTE] k=7, s=1, alpha=0.60 -> Hamming=0.0964, F1m=0.2553\n",
      "[MLSMOTE] k=7, s=1, alpha=0.90 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=1, alpha=1.00 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=7, s=2, alpha=0.00 -> Hamming=0.0744, F1m=0.2286\n",
      "[MLSMOTE] k=7, s=2, alpha=0.30 -> Hamming=0.0964, F1m=0.2553\n",
      "[MLSMOTE] k=7, s=2, alpha=0.60 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=2, alpha=0.90 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=2, alpha=1.00 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=3, alpha=0.00 -> Hamming=0.0496, F1m=0.3077\n",
      "[MLSMOTE] k=7, s=3, alpha=0.30 -> Hamming=0.0964, F1m=0.2553\n",
      "[MLSMOTE] k=7, s=3, alpha=0.60 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=3, alpha=0.90 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=3, alpha=1.00 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=4, alpha=0.00 -> Hamming=0.0496, F1m=0.3077\n",
      "[MLSMOTE] k=7, s=4, alpha=0.30 -> Hamming=0.0964, F1m=0.2553\n",
      "[MLSMOTE] k=7, s=4, alpha=0.60 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=4, alpha=0.90 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=4, alpha=1.00 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=5, alpha=0.00 -> Hamming=0.0496, F1m=0.3077\n",
      "[MLSMOTE] k=7, s=5, alpha=0.30 -> Hamming=0.0964, F1m=0.2553\n",
      "[MLSMOTE] k=7, s=5, alpha=0.60 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=5, alpha=0.90 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=7, s=5, alpha=1.00 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=9, s=1, alpha=0.00 -> Hamming=0.0606, F1m=0.2143\n",
      "[MLSMOTE] k=9, s=1, alpha=0.30 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=9, s=1, alpha=0.60 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=1, alpha=0.90 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=1, alpha=1.00 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=2, alpha=0.00 -> Hamming=0.0606, F1m=0.2143\n",
      "[MLSMOTE] k=9, s=2, alpha=0.30 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=9, s=2, alpha=0.60 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=2, alpha=0.90 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=2, alpha=1.00 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=3, alpha=0.00 -> Hamming=0.0606, F1m=0.2143\n",
      "[MLSMOTE] k=9, s=3, alpha=0.30 -> Hamming=0.0937, F1m=0.2917\n",
      "[MLSMOTE] k=9, s=3, alpha=0.60 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=9, s=3, alpha=0.90 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=3, alpha=1.00 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=4, alpha=0.00 -> Hamming=0.0661, F1m=0.0769\n",
      "[MLSMOTE] k=9, s=4, alpha=0.30 -> Hamming=0.0909, F1m=0.2979\n",
      "[MLSMOTE] k=9, s=4, alpha=0.60 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=9, s=4, alpha=0.90 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=4, alpha=1.00 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=5, alpha=0.00 -> Hamming=0.0661, F1m=0.0769\n",
      "[MLSMOTE] k=9, s=5, alpha=0.30 -> Hamming=0.0854, F1m=0.3111\n",
      "[MLSMOTE] k=9, s=5, alpha=0.60 -> Hamming=0.0964, F1m=0.2857\n",
      "[MLSMOTE] k=9, s=5, alpha=0.90 -> Hamming=0.0992, F1m=0.2800\n",
      "[MLSMOTE] k=9, s=5, alpha=1.00 -> Hamming=0.0992, F1m=0.2800\n",
      "\n",
      "=== 最优（有MLSMOTE）参数与指标 ===\n",
      "参数 (mode,k,s,alpha): ('mls', 4, 1, 0.0)\n",
      "Hamming Loss: 0.0579\n",
      "Subset Accuracy: 0.8595\n",
      "Precision (micro): 0.4375\n",
      "Recall (micro): 0.3684\n",
      "F1-score (micro): 0.4000\n",
      "Precision (macro): 0.4452\n",
      "Recall (macro): 0.3889\n",
      "F1-score (macro): 0.3733\n",
      "[saved] MLKNN(mls) 参数 -> mlknn_parameter_data\\mlknn_params_weighted_prior.npz\n",
      "[saved] MLKNN(mls) 实验结果 -> mlknn_parameter_data\\mlknn_experiment_weighted_prior.npz\n",
      "\n",
      "=== 运行：MLKNN (无 MLSMOTE) 网格搜索 ===\n",
      "[No-MLSMOTE] k=3, s=1, alpha=0.00 -> Hamming=0.0413, F1m=0.4000\n",
      "[No-MLSMOTE] k=3, s=1, alpha=0.30 -> Hamming=0.0413, F1m=0.4828\n",
      "[No-MLSMOTE] k=3, s=1, alpha=0.60 -> Hamming=0.0413, F1m=0.5161\n",
      "[No-MLSMOTE] k=3, s=1, alpha=0.90 -> Hamming=0.0964, F1m=0.3137\n",
      "[No-MLSMOTE] k=3, s=1, alpha=1.00 -> Hamming=0.1019, F1m=0.3019\n",
      "[No-MLSMOTE] k=3, s=2, alpha=0.00 -> Hamming=0.0413, F1m=0.4000\n",
      "[No-MLSMOTE] k=3, s=2, alpha=0.30 -> Hamming=0.0413, F1m=0.4828\n",
      "[No-MLSMOTE] k=3, s=2, alpha=0.60 -> Hamming=0.0523, F1m=0.4571\n",
      "[No-MLSMOTE] k=3, s=2, alpha=0.90 -> Hamming=0.0909, F1m=0.3265\n",
      "[No-MLSMOTE] k=3, s=2, alpha=1.00 -> Hamming=0.1019, F1m=0.3019\n",
      "[No-MLSMOTE] k=3, s=3, alpha=0.00 -> Hamming=0.0413, F1m=0.4000\n",
      "[No-MLSMOTE] k=3, s=3, alpha=0.30 -> Hamming=0.0413, F1m=0.4828\n",
      "[No-MLSMOTE] k=3, s=3, alpha=0.60 -> Hamming=0.0634, F1m=0.4103\n",
      "[No-MLSMOTE] k=3, s=3, alpha=0.90 -> Hamming=0.0909, F1m=0.3265\n",
      "[No-MLSMOTE] k=3, s=3, alpha=1.00 -> Hamming=0.1019, F1m=0.3019\n",
      "[No-MLSMOTE] k=3, s=4, alpha=0.00 -> Hamming=0.0413, F1m=0.3478\n",
      "[No-MLSMOTE] k=3, s=4, alpha=0.30 -> Hamming=0.0413, F1m=0.4828\n",
      "[No-MLSMOTE] k=3, s=4, alpha=0.60 -> Hamming=0.0661, F1m=0.4000\n",
      "[No-MLSMOTE] k=3, s=4, alpha=0.90 -> Hamming=0.0882, F1m=0.3333\n",
      "[No-MLSMOTE] k=3, s=4, alpha=1.00 -> Hamming=0.1019, F1m=0.3019\n",
      "[No-MLSMOTE] k=3, s=5, alpha=0.00 -> Hamming=0.0413, F1m=0.3478\n",
      "[No-MLSMOTE] k=3, s=5, alpha=0.30 -> Hamming=0.0413, F1m=0.4828\n",
      "[No-MLSMOTE] k=3, s=5, alpha=0.60 -> Hamming=0.0661, F1m=0.4000\n",
      "[No-MLSMOTE] k=3, s=5, alpha=0.90 -> Hamming=0.0826, F1m=0.3478\n",
      "[No-MLSMOTE] k=3, s=5, alpha=1.00 -> Hamming=0.0992, F1m=0.3077\n",
      "[No-MLSMOTE] k=4, s=1, alpha=0.00 -> Hamming=0.0468, F1m=0.3704\n",
      "[No-MLSMOTE] k=4, s=1, alpha=0.30 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=1, alpha=0.60 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=1, alpha=0.90 -> Hamming=0.0496, F1m=0.4375\n",
      "[No-MLSMOTE] k=4, s=1, alpha=1.00 -> Hamming=0.0606, F1m=0.3889\n",
      "[No-MLSMOTE] k=4, s=2, alpha=0.00 -> Hamming=0.0468, F1m=0.3704\n",
      "[No-MLSMOTE] k=4, s=2, alpha=0.30 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=2, alpha=0.60 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=2, alpha=0.90 -> Hamming=0.0496, F1m=0.4375\n",
      "[No-MLSMOTE] k=4, s=2, alpha=1.00 -> Hamming=0.0551, F1m=0.4444\n",
      "[No-MLSMOTE] k=4, s=3, alpha=0.00 -> Hamming=0.0468, F1m=0.3704\n",
      "[No-MLSMOTE] k=4, s=3, alpha=0.30 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=3, alpha=0.60 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=3, alpha=0.90 -> Hamming=0.0441, F1m=0.5000\n",
      "[No-MLSMOTE] k=4, s=3, alpha=1.00 -> Hamming=0.0579, F1m=0.4324\n",
      "[No-MLSMOTE] k=4, s=4, alpha=0.00 -> Hamming=0.0496, F1m=0.1818\n",
      "[No-MLSMOTE] k=4, s=4, alpha=0.30 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=4, alpha=0.60 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=4, alpha=0.90 -> Hamming=0.0468, F1m=0.4848\n",
      "[No-MLSMOTE] k=4, s=4, alpha=1.00 -> Hamming=0.0579, F1m=0.4324\n",
      "[No-MLSMOTE] k=4, s=5, alpha=0.00 -> Hamming=0.0523, F1m=0.0000\n",
      "[No-MLSMOTE] k=4, s=5, alpha=0.30 -> Hamming=0.0468, F1m=0.4516\n",
      "[No-MLSMOTE] k=4, s=5, alpha=0.60 -> Hamming=0.0468, F1m=0.4516\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------ 多标签评估函数 ------------------\n",
    "def evaluate_multilabel(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"Hamming Loss\": hamming_loss(y_true, y_pred),\n",
    "        \"Subset Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision (micro)\": precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        \"Recall (micro)\": recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        \"F1-score (micro)\": f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        \"Precision (macro)\": precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        \"Recall (macro)\": recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        \"F1-score (macro)\": f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ------------------ 改进版 MLKNN：距离加权 + 局部先验校正 ------------------\n",
    "class MLKNN_WeightedPrior:\n",
    "    def __init__(self, train_x, train_y, k=10, s=1,\n",
    "                 distance_weight=True, prior_correction=True, prior_alpha=0.1, sigma=None):\n",
    "        \"\"\"\n",
    "        train_x: numpy array or pd.DataFrame\n",
    "        train_y: numpy array or pd.DataFrame (binary labels 0/1)\n",
    "        k: neighbors\n",
    "        s: smooth parameter for Laplace smoothing in Peh\n",
    "        distance_weight: whether to use distance weighting for neighbor contribution\n",
    "        prior_correction: whether to adjust global prior Ph1 by local prior\n",
    "        prior_alpha: interpolation factor between global prior and local prior (0..1)\n",
    "        sigma: Gaussian kernel width; if None estimate per query as median distance (fallback)\n",
    "        \"\"\"\n",
    "        self.k = int(k)\n",
    "        self.s = s\n",
    "        self.distance_weight = bool(distance_weight)\n",
    "        self.prior_correction = bool(prior_correction)\n",
    "        self.prior_alpha = float(prior_alpha)\n",
    "        self.sigma = sigma  # can be None\n",
    "        self.train_x = np.array(train_x)\n",
    "        self.train_y = np.array(train_y).astype(int)\n",
    "        self.label_num = self.train_y.shape[1]\n",
    "        self.train_data_num = self.train_x.shape[0]\n",
    "\n",
    "        # Prior and conditional probability tables (same shape as classical MLkNN)\n",
    "        self.Ph1 = np.zeros(self.label_num)\n",
    "        self.Ph0 = np.zeros(self.label_num)\n",
    "        self.Peh1 = np.zeros((self.label_num, self.k + 1))\n",
    "        self.Peh0 = np.zeros((self.label_num, self.k + 1))\n",
    "\n",
    "        # prepare nearest neighbors structure\n",
    "        self.nbrs = NearestNeighbors(metric='euclidean').fit(self.train_x)\n",
    "\n",
    "    def train(self):\n",
    "        # 1) 全局先验\n",
    "        for i in range(self.label_num):\n",
    "            cnt = np.sum(self.train_y[:, i])\n",
    "            self.Ph1[i] = (self.s + cnt) / (self.s * 2 + self.train_data_num)\n",
    "            self.Ph0[i] = 1.0 - self.Ph1[i]\n",
    "\n",
    "        # 2) 条件概率 Peh1/Peh0: 对每个训练样本取其 k 邻居并统计邻居中标签为1的个数\n",
    "        for i in range(self.label_num):\n",
    "            c1 = np.zeros(self.k + 1, dtype=float)\n",
    "            c0 = np.zeros(self.k + 1, dtype=float)\n",
    "            for idx in range(self.train_data_num):\n",
    "                # note: kneighbors returns self-point included if present; but using training point as query includes itself;\n",
    "                # classical MLkNN counts neighbors excluding self — we keep consistency by requesting k+1 and removing itself if appears.\n",
    "                dists, inds = self.nbrs.kneighbors([self.train_x[idx]], n_neighbors=self.k + 1)\n",
    "                inds = inds[0].tolist()\n",
    "                # remove self index if present\n",
    "                if idx in inds:\n",
    "                    inds.remove(idx)\n",
    "                # if after removal we have >k keep first k, otherwise use as many as available\n",
    "                inds = inds[:self.k]\n",
    "                temp = int(np.sum(self.train_y[inds, i]))\n",
    "                if self.train_y[idx, i] == 1:\n",
    "                    c1[temp] += 1\n",
    "                else:\n",
    "                    c0[temp] += 1\n",
    "            # Laplace-smoothed conditional probabilities\n",
    "            denom1 = (self.s * (self.k + 1) + np.sum(c1))\n",
    "            denom0 = (self.s * (self.k + 1) + np.sum(c0))\n",
    "            for cnt in range(self.k + 1):\n",
    "                self.Peh1[i, cnt] = (self.s + c1[cnt]) / denom1\n",
    "                self.Peh0[i, cnt] = (self.s + c0[cnt]) / denom0\n",
    "\n",
    "    def _compute_weights(self, distances):\n",
    "        # distances: 1D array of length k (or <=k)\n",
    "        # use Gaussian kernel weights; if sigma is None, fallback to median(distances) or 1e-3\n",
    "        if not self.distance_weight:\n",
    "            return np.ones_like(distances, dtype=float)\n",
    "        if self.sigma is None:\n",
    "            sigma = np.median(distances) if np.median(distances) > 1e-6 else (np.mean(distances) + 1e-6)\n",
    "        else:\n",
    "            sigma = self.sigma\n",
    "        # Gaussian kernel on Euclidean distance; larger dist -> smaller weight\n",
    "        w = np.exp(- (distances ** 2) / (2.0 * (sigma ** 2) + 1e-12))\n",
    "        # avoid all-zeros\n",
    "        if w.sum() == 0:\n",
    "            w = np.ones_like(w)\n",
    "        return w\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        test_x = np.array(test_x)\n",
    "        n_test = test_x.shape[0]\n",
    "        y_pred = np.zeros((n_test, self.label_num), dtype=int)\n",
    "\n",
    "        # for each test sample, find neighbors, compute weighted label sum & local prior, then decide\n",
    "        for t in range(n_test):\n",
    "            dists, inds = self.nbrs.kneighbors([test_x[t]], n_neighbors=self.k + 1)\n",
    "            inds = inds[0].tolist()\n",
    "            dists = dists[0]\n",
    "            # Remove self if present (defensive, though test point not in train generally)\n",
    "            # keep first k neighbors after removal\n",
    "            if len(inds) > 0 and inds[0] == t:\n",
    "                inds = inds[1:]\n",
    "                dists = dists[1:]\n",
    "            inds = inds[:self.k]\n",
    "            dists = dists[:len(inds)]\n",
    "            if len(inds) == 0:\n",
    "                # fallback: predict all zeros if no neighbors\n",
    "                continue\n",
    "\n",
    "            weights = self._compute_weights(np.array(dists))\n",
    "            weights = weights[:len(inds)]\n",
    "            wsum = np.sum(weights) if np.sum(weights) > 0 else 1.0\n",
    "\n",
    "            for label_idx in range(self.label_num):\n",
    "                neighbor_labels = self.train_y[inds, label_idx]  # 0/1 array\n",
    "                # weighted proportion of positive labels among neighbors\n",
    "                p_local = float(np.sum(weights * neighbor_labels) / wsum)\n",
    "\n",
    "                # adjusted prior: interpolate between global prior and local prior\n",
    "                if self.prior_correction:\n",
    "                    Ph1_adj = (1.0 - self.prior_alpha) * self.Ph1[label_idx] + self.prior_alpha * p_local\n",
    "                    Ph0_adj = 1.0 - Ph1_adj\n",
    "                else:\n",
    "                    Ph1_adj = self.Ph1[label_idx]\n",
    "                    Ph0_adj = self.Ph0[label_idx]\n",
    "\n",
    "                # Map weighted positive mass to an integer count for Peh lookup\n",
    "                # scale = expected count = p_local * k_effective\n",
    "                k_eff = max(1, len(inds))\n",
    "                expected_count = p_local * k_eff\n",
    "                cnt_idx = int(round(expected_count))\n",
    "                cnt_idx = min(max(0, cnt_idx), self.k)  # ensure in [0,k]\n",
    "\n",
    "                # Bayes decision with adjusted prior\n",
    "                prob1 = Ph1_adj * self.Peh1[label_idx, cnt_idx]\n",
    "                prob0 = Ph0_adj * self.Peh0[label_idx, cnt_idx]\n",
    "\n",
    "                if prob1 > prob0:\n",
    "                    y_pred[t, label_idx] = 1\n",
    "                else:\n",
    "                    y_pred[t, label_idx] = 0\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "# ------------------ 主流程（网格搜索 + 保存） ------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # === 超参数列表 ===\n",
    "    k_list = [3, 4, 5, 7, 9]\n",
    "    s_list = [1, 2, 3, 4, 5]\n",
    "    prior_alphas = [0.0, 0.3, 0.6, 0.9,1.0]   # 0.0 = 不使用局部先验，1.0 = 完全使用局部先验\n",
    "    best_score = -1.0\n",
    "    best_entry = None\n",
    "\n",
    "    # ---------- 有 MLSMOTE 训练数据 ----------\n",
    "    print(\"=== 运行：MLKNN (有 MLSMOTE) 网格搜索 ===\")\n",
    "    best_model_mls = None\n",
    "    best_pred_mls = None\n",
    "    best_metrics_mls = None\n",
    "\n",
    "    for k in k_list:\n",
    "        for s in s_list:\n",
    "            for alpha in prior_alphas:\n",
    "                model = MLKNN_WeightedPrior(train_x_res_scaled, train_y_res.values,\n",
    "                                            k=k, s=s, distance_weight=True,\n",
    "                                            prior_correction=True, prior_alpha=alpha, sigma=None)\n",
    "                model.train()\n",
    "                y_pred = model.predict(test_x_selected)\n",
    "                metrics = evaluate_multilabel(test_y.values, y_pred)\n",
    "                f1m = metrics[\"F1-score (micro)\"]\n",
    "                print(f\"[MLSMOTE] k={k}, s={s}, alpha={alpha:.2f} -> Hamming={metrics['Hamming Loss']:.4f}, F1m={f1m:.4f}\")\n",
    "                # 以 F1-micro 为主要选取依据（可根据需要改）\n",
    "                if f1m > best_score:\n",
    "                    best_score = f1m\n",
    "                    best_model_mls = model\n",
    "                    best_pred_mls = y_pred\n",
    "                    best_metrics_mls = metrics\n",
    "                    best_entry = (\"mls\", k, s, alpha)\n",
    "\n",
    "    print(\"\\n=== 最优（有MLSMOTE）参数与指标 ===\")\n",
    "    print(\"参数 (mode,k,s,alpha):\", best_entry)\n",
    "    for k, v in best_metrics_mls.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    # 保存\n",
    "    os.makedirs('mlknn_parameter_data', exist_ok=True)\n",
    "    param_filepath = os.path.join('mlknn_parameter_data', 'mlknn_params_weighted_prior.npz')\n",
    "    np.savez_compressed(\n",
    "        param_filepath,\n",
    "        Ph1=best_model_mls.Ph1,\n",
    "        Ph0=best_model_mls.Ph0,\n",
    "        Peh1=best_model_mls.Peh1,\n",
    "        Peh0=best_model_mls.Peh0,\n",
    "        k=np.array([best_model_mls.k]),\n",
    "        s=np.array([best_model_mls.s]),\n",
    "        prior_alpha=np.array([best_model_mls.prior_alpha])\n",
    "    )\n",
    "    print(f\"[saved] MLKNN(mls) 参数 -> {param_filepath}\")\n",
    "\n",
    "    exper_path = os.path.join('mlknn_parameter_data', 'mlknn_experiment_weighted_prior.npz')\n",
    "    np.savez_compressed(\n",
    "        exper_path,\n",
    "        predict=best_pred_mls,\n",
    "        y_true=test_y.values,\n",
    "        metrics=best_metrics_mls\n",
    "    )\n",
    "    print(f\"[saved] MLKNN(mls) 实验结果 -> {exper_path}\")\n",
    "\n",
    "    # ---------- 无 MLSMOTE 对照 ----------\n",
    "    print(\"\\n=== 运行：MLKNN (无 MLSMOTE) 网格搜索 ===\")\n",
    "    best_model_nomls = None\n",
    "    best_pred_nomls = None\n",
    "    best_metrics_nomls = None\n",
    "    best_score_nomls = -1.0\n",
    "    best_entry_nomls = None\n",
    "\n",
    "    for k in k_list:\n",
    "        for s in s_list:\n",
    "            for alpha in prior_alphas:\n",
    "                model = MLKNN_WeightedPrior(train_x_selected, train_y.values,\n",
    "                                            k=k, s=s, distance_weight=True,\n",
    "                                            prior_correction=True, prior_alpha=alpha, sigma=None)\n",
    "                model.train()\n",
    "                y_pred = model.predict(test_x_selected)\n",
    "                metrics = evaluate_multilabel(test_y.values, y_pred)\n",
    "                f1m = metrics[\"F1-score (micro)\"]\n",
    "                print(f\"[No-MLSMOTE] k={k}, s={s}, alpha={alpha:.2f} -> Hamming={metrics['Hamming Loss']:.4f}, F1m={f1m:.4f}\")\n",
    "                if f1m > best_score_nomls:\n",
    "                    best_score_nomls = f1m\n",
    "                    best_model_nomls = model\n",
    "                    best_pred_nomls = y_pred\n",
    "                    best_metrics_nomls = metrics\n",
    "                    best_entry_nomls = (\"nomls\", k, s, alpha)\n",
    "\n",
    "    print(\"\\n=== 最优（无MLSMOTE）参数与指标 ===\")\n",
    "    print(\"参数 (mode,k,s,alpha):\", best_entry_nomls)\n",
    "    for k, v in best_metrics_nomls.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    # 保存 noml\n",
    "    os.makedirs('nomlknn_parameter_data', exist_ok=True)\n",
    "    param_filepath2 = os.path.join('nomlknn_parameter_data', 'nomlknn_params_weighted_prior.npz')\n",
    "    np.savez_compressed(\n",
    "        param_filepath2,\n",
    "        Ph1=best_model_nomls.Ph1,\n",
    "        Ph0=best_model_nomls.Ph0,\n",
    "        Peh1=best_model_nomls.Peh1,\n",
    "        Peh0=best_model_nomls.Peh0,\n",
    "        k=np.array([best_model_nomls.k]),\n",
    "        s=np.array([best_model_nomls.s]),\n",
    "        prior_alpha=np.array([best_model_nomls.prior_alpha])\n",
    "    )\n",
    "    print(f\"[saved] MLKNN(nomls) 参数 -> {param_filepath2}\")\n",
    "\n",
    "    exper_path2 = os.path.join('nomlknn_parameter_data', 'nomlknn_experiment_weighted_prior.npz')\n",
    "    np.savez_compressed(\n",
    "        exper_path2,\n",
    "        predict=best_pred_nomls,\n",
    "        y_true=test_y.values,\n",
    "        metrics=best_metrics_nomls\n",
    "    )\n",
    "    print(f\"[saved] MLKNN(noml) 实验结果 -> {exper_path2}\")\n",
    "\n",
    "    # 最后打印对比 summary\n",
    "    print(\"\\n=== 对比 Summary ===\")\n",
    "    print(\"有MLSMOTE 最优指标:\")\n",
    "    for k, v in best_metrics_mls.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    print(\"\\n无MLSMOTE 最优指标:\")\n",
    "    for k, v in best_metrics_nomls.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "084d3f1b-6be3-4310-b4a0-9f3d4d1eacfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLRF Hamming Loss=0.0551\n",
      "\n",
      "=== 多标签随机森林模型评估指标 ===\n",
      "Hamming Loss: 0.0551\n",
      "Subset Accuracy: 0.8595\n",
      "Precision (micro): 0.4286\n",
      "Recall (micro): 0.1579\n",
      "F1-score (micro): 0.2308\n",
      "Precision (macro): 0.2500\n",
      "Recall (macro): 0.0833\n",
      "F1-score (macro): 0.1250\n",
      "[saved] MLRF 实验结果 -> mlrf_parameter_data\\mlrf_experiment.npz\n",
      "MLRF Hamming Loss=0.0496\n",
      "\n",
      "=== 多标签随机森林模型评估指标 ===\n",
      "Hamming Loss: 0.0496\n",
      "Subset Accuracy: 0.8760\n",
      "Precision (micro): 0.6000\n",
      "Recall (micro): 0.1579\n",
      "F1-score (micro): 0.2500\n",
      "Precision (macro): 0.3333\n",
      "Recall (macro): 0.0833\n",
      "F1-score (macro): 0.1333\n",
      "[saved] 无过采样MLRF 实验结果 -> nomlrf_parameter_data\\nomlrf_experiment.npz\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "# --------- 多标签随机森林模型 ----------\n",
    "def run_ml_random_forest(train_x, train_y, test_x, test_y):\n",
    "    # 随机森林本身支持多标签，也可以用 MultiOutputClassifier 封装\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # 用 MultiOutputClassifier 包装（更保险，尤其在多标签 one-hot 情况下）\n",
    "    clf = MultiOutputClassifier(rf, n_jobs=-1)\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    # 预测\n",
    "    y_pred = clf.predict(test_x)\n",
    "    # 计算 Hamming Loss\n",
    "    loss = hamming_loss(test_y.values, y_pred)\n",
    "    print(f\"MLRF Hamming Loss={loss:.4f}\")\n",
    "\n",
    "    # 计算其他评估指标\n",
    "    metrics = evaluate_multilabel(test_y.values, y_pred)\n",
    "    print(\"\\n=== 多标签随机森林模型评估指标 ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    return clf, y_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model, best_predict = run_ml_random_forest(train_x_res_scaled, train_y_res.values, test_x_selected, test_y)\n",
    "    os.makedirs('mlrf_parameter_data', exist_ok=True)\n",
    "\n",
    "    experiment_filepath = os.path.join('mlrf_parameter_data', 'mlrf_experiment.npz')\n",
    "    np.savez_compressed(\n",
    "        experiment_filepath,\n",
    "        predict=best_predict,\n",
    "        y_true=test_y.values,\n",
    "        hamming_loss=np.array([hamming_loss(test_y.values, best_predict)])\n",
    "    )\n",
    "    print(f\"[saved] MLRF 实验结果 -> {experiment_filepath}\")\n",
    "\n",
    "\n",
    "##NOSMOTE\n",
    "    model1, best_predict1 = run_ml_random_forest(train_x_selected, train_y, test_x_selected, test_y)\n",
    "\n",
    "\n",
    "    os.makedirs('nomlrf_parameter_data', exist_ok=True)\n",
    "\n",
    "    experiment_filepath1 = os.path.join('nomlrf_parameter_data', 'nomlrf_experiment.npz')\n",
    "    np.savez_compressed(\n",
    "        experiment_filepath1,\n",
    "        predict=best_predict1,\n",
    "        y_true=test_y.values,\n",
    "        hamming_loss=np.array([hamming_loss(test_y.values, best_predict)])\n",
    "    )\n",
    "    print(f\"[saved] 无过采样MLRF 实验结果 -> {experiment_filepath1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53ddcc5b-cd02-46c7-8c23-8702f1d3a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akuta\\AppData\\Local\\Temp\\ipykernel_15352\\1962234394.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pos_weights = torch.tensor((neg_counts / (pos_counts + 1e-6)) * 0.5, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 0.6315\n",
      "Epoch 20/200, Loss: 0.5305\n",
      "Epoch 30/200, Loss: 0.4820\n",
      "Epoch 40/200, Loss: 0.4173\n",
      "Epoch 50/200, Loss: 0.3722\n",
      "Epoch 60/200, Loss: 0.3611\n",
      "Epoch 70/200, Loss: 0.3117\n",
      "Epoch 80/200, Loss: 0.2948\n",
      "Epoch 90/200, Loss: 0.2897\n",
      "Epoch 100/200, Loss: 0.2672\n",
      "Epoch 110/200, Loss: 0.2618\n",
      "Epoch 120/200, Loss: 0.2414\n",
      "Epoch 130/200, Loss: 0.2255\n",
      "Epoch 140/200, Loss: 0.2402\n",
      "Epoch 150/200, Loss: 0.2236\n",
      "Epoch 160/200, Loss: 0.2394\n",
      "Epoch 170/200, Loss: 0.2011\n",
      "Epoch 180/200, Loss: 0.2176\n",
      "Epoch 190/200, Loss: 0.2256\n",
      "Epoch 200/200, Loss: 0.2160\n",
      "Best thresholds per label: [0.5 0.5 0.5]\n",
      "\n",
      "=== MLP + Weighted BCE + Optimized Threshold 模型评估指标 ===\n",
      "Hamming Loss: 0.0937\n",
      "Subset Accuracy: 0.7686\n",
      "Precision (micro): 0.2414\n",
      "Recall (micro): 0.3684\n",
      "F1-score (micro): 0.2917\n",
      "Precision (macro): 0.1795\n",
      "Recall (macro): 0.3056\n",
      "F1-score (macro): 0.2118\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApcAAAHlCAYAAACgbtfGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwRklEQVR4nO3deVRV5cLH8R+DgIiMoiXhhFqpEUreyNI0rTRRbLhlWibWWw5l3pTM22BlqWXXtMEIM02XadbtmtfZnKJBjSy8iJoTCvqWEx6Q4SCw3z9c7utJJMGnGN7vZ62z1t377L15Dvbc9WWfs/dxsyzLEgAAAGCAe1UPAAAAALUHcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQmg0rZv3/6HHfvgwYPasWPHRW2bnZ2t4uJie7mkpEQHDhy46J/13nvv6fvvv6/wGM/15ptv6tVXX9Vvv/SsoKBATzzxxEW9lqysLC1fvvy89fv27VNRUdElja+inE6n/b9zc3O1YcOGCh/DsiwNGjRIq1evNjgyANUdcQmgUpxOp3r16qVHHnlEJSUl9voTJ04oLy+vzH2+/fZbHT9+3F7OzMxUTExMmeG1aNEide3aVfn5+b87lueff14xMTF2gP3jH/9QdHS0fv7554t6Le+8845++OEHe3n79u3at2+fsrKyXB4HDhwo85iHDx/W+PHjtXbtWn3xxRf617/+pYULF0qSEhMTNWPGDP3yyy/KyspSZmam9uzZo9OnT0s6E5QxMTE6dOiQvv76aw0ZMkSS9Pjjj+sf//iHJGnYsGEaMGBAua/hf//3f3XZZZcpMjJSUVFRF3w0btxY69evL/dYmzZtUkREhP3Hw7Jly3Tbbbfp4MGDF/X7POu7777TvHnzXP7NAfw/YAFAJe3cudO6/PLLrfvvv98qKSmxLMuypk6darVp08ayLMs6cuSI9cknn9jbN2nSxFq9erW9PGnSJKtx48bWqVOnrJKSEis3N9cqLi62LMuyOnXqZL3wwgv2tk6n0zp58uR5Y0hPT7e8vLysjz/+2F53+vRp66677rKWLVtmryspKbHy8vLs45+rQ4cO1ocffmgvBwQEWJLKfDRt2tRl36KiIqt79+5WYGCg1bBhQysgIMBq166d1bZtW+vw4cNWgwYNrHr16lk+Pj6Wl5eXFRAQYHl5eVkZGRn2MZ544gnrtttus/75z39a4eHh1pEjR6z69etbaWlplmVZVsOGDa2PPvqonH8Jyzp69KglycrMzCx3u4CAAGv9+vXlbnPHHXdYV111lf27KikpsSIjI6277rrrgvtkZ2db+fn5Lr/fgQMHWq1atbL/2zhXSUmJVVhYaOXl5ZU7FgA1j2cVdi2AGu7KK6/UmjVrdNttt2nbtm2KioqSr6+v6tevL0nKyMjQgAEDdO+990qS6tSpozp16kg689Z1YmKiXn31VdWrV0+7d+9W69atXY7/7bff6uWXX7aXPTw8XN7+djqdeuCBB1RSUqL4+HjFx8dLkoqLi1VaWqply5adN+avvvpKf/nLX1zWubm5yc3NzV7etWuX6tevL19fX3tdSUmJcnNzXd4uPnbsmAYNGqQffvhBycnJ2rZtm8aMGaOtW7fq9OnT6ty5syIiIrR+/Xp99913evDBB7V//355eXm5vMaoqCi1aNFCW7ZsUWFhoZYuXaq+fftq69atKikp0dGjR9WrVy97n9OnT8uyLJfjuLufeSOqa9eu8vS88P+15+bmysPD44LPJycna/ny5Vq8eLG9nbu7u9566y1169ZNs2fPtn/P5woKCrrgMcv7effdd599lhdA7UBcArgkbdu21a5du+Tn5ydJ8vLyskPnbEiW5dNPP5Wfn58GDRqk9PR0xcXFadu2bYqIiFBiYqLmzZun7777zt6+tLRUhYWFLscYPny4tm7dqpEjR+rVV1+11z/77LPaunWrVqxYIenMZ/+cTqeCgoLs0Pnhhx+UkJAgLy8v7dmzR2+++abmz5+v2NhYPfnkk+eN18PDQ4GBgS7rdu/erezsbK1bt07t2rVTo0aNNG3aNB05ckTDhg1Tnz59NGLECNWtW1c33XSTevbsqZMnT6phw4b2Mb766iutWLFCp06d0tatW+Xj46OFCxeqtLRUM2fOVGxsrCzLctlHkiZMmKDnnnvOXj77kYARI0aUG3rDhw+/4Oc38/LyFB8fr+7duysuLs7luZtvvlkjR47U0KFD1ahRI91xxx0uz6enp8vb21teXl7y9PTU/fffr9zcXC1dulRbt27V8ePHdeutt573M+vWrXvBsQKomYhLABW2Z88eTZgwQT4+PnJzc9MTTzyhtm3bSjoTYdZvLmr5rYKCAj3//POaNm2a3N3dNWPGDIWFhSk0NFTFxcVavHix7rvvPhUXF8vDw8MOkHPPJE6bNk3z589XdHS0Zs6cqfnz59vP5efn6/Tp02rWrJmkM3FZUFCgHTt2qGnTppKkyy+/XEOHDtXhw4e1atUqde/eXV26dJGnp6f27NlzwbN/xcXF8vHx0RVXXKEbbrjBJYBDQ0PVr18/9e7dWwcOHNDGjRvVrFkzDR48WF5eXpo1a9Z5x3vmmWc0bNgw3XHHHbryyitVUFCgrl27Ki4uTm3atFFUVJTGjBmj8ePHS5I6dOigsWPHqn///pLOnMUsLS2Vv7+/FixYUO7vXZI+/PBDtWnTxv7M57l/AIwcOVLHjh3Tl19+Wea+U6ZM0cGDBxUXF6dJkyZp9OjR9hnfq6++2t7un//8p5KTk7V582ZddtllWr9+vb7//ns9+OCDvzs+ADUfcQmgwjw9PRUSEqK6detq4sSJuvvuu+24/O3ZxbKMHz9e2dnZSk9PV0pKiubOnastW7aoadOm8vLyUl5entLT0/XKK6+offv2Sk5OPu8Y+fn5+uCDD/Tll1/qxhtvLPfMpXTmzN65Z8kaN26se++9V0lJSZKka665RjfddJOeeeYZzZo1S15eXnJzc1NxcbEsy7IjrKioSPfee2+Zb+U6HA799a9/1X/+8x9t2bJFixYtUnx8vD766CONGzdOt91223n7bN68WYMHD9Y111yjUaNGadSoUQoICNCtt96qd999V6mpqerWrZt9ZvjXX39V06ZNVa9ePUnSnDlzNHLkSPn4+CggIEA+Pj7l/u5LS0vlcDiUm5urxMREDRo0SJL08ssva/bs2Zo6daosy1JmZqbLRwXOevPNN1WvXj0lJCTos88+0/z58xUREWE/v3PnTj388MN6+eWXFR0dLelMwHp7e5c7LgC1h5v1e6cYAKAcbm5u2rBhg26++WZJ0vTp0/X5559r48aN+umnn3TdddfZn5Ns2bKlPvjgA1mWpQULFqhNmzZ666239OCDD+qll16SJI0bN05ZWVmaN2+epk2bpqVLl17wTJokDR48WAsXLnSJqsLCQhUXF9tBJp2Jwvfee08PPfSQy/49evTQ2rVr1apVKzVu3FjLly9X3bp17bB65JFHVFxcrDlz5tj7lJSUnPc5wmXLlunxxx9Xfn6+1qxZo8jISElnrkQfNWqUSkpK1KJFC/Xu3VsxMTGKjo5WixYtNHjwYLVu3VovvPCCfUuk2bNn64MPPtC//vUv+zOZ69evV35+vurVq6c9e/a4BJ105mrxL7744qL+zbp06aI2bdrYyytXrlSvXr302muvad26dVq1alW5+8+cOVN16tTRvHnztHz5cvuznwcPHlTXrl21f//+ixrHmjVr1KNHj4vaFkDNwa2IAFyyc0Pr119/Pe+zib/VrVs3JSUlKSIiQr6+vvr73/8uSfrmm2/01ltv6dFHH1VWVpYcDoecTqeysrK0b9++Mm+FU1RUpMcee8zllkGPPfaYbrjhBpd1hw8fVs+ePV323b59uzZs2KDg4GANGDBAmzdv1pIlS5STk6OjR4+e97N27tzp8nozMjL09ttvq0OHDoqNjVVeXp7ef/991alTRzt37tTOnTvVvXt3TZ8+XQ0aNNC+ffv09ttva+DAgdq2bZvq1KmjxMREjRkzRnv37tWIESM0cOBAFRcX68EHH5SXl5feeecdbdq0SU6nU7t27ZKvr6/9dv+5Dhw4oGHDhsnNzU0+Pj7y8fHRypUrNWvWLHvZx8dHzz33nLZs2eKyb8+ePfX999/r6aef1oIFC5Sdna0ZM2bIy8tLJ0+eVG5urnJzc+1obNasmR566CF9+eWXdlimp6erc+fOCggIUIsWLTRhwgRlZmYqMzNTI0eOVNeuXe3ljIwMSeJsJlBL8bY4AKO2b99eZvz81oEDBzR8+HAtW7ZM3t7eKikp0apVq+R0OhUXFyd3d3cVFhaqqKhIUVFRKigo0E033XTeWTWHw6HPP/9cH330kaQzn6/MycmRt7e3rrjiCnu7kpISFRQU6NSpU/Lx8ZFlWRo9erTi4uJ04MABNWvWTH/729/k4eGhWbNm6fnnn3e5EfuyZcvUt29fffXVV7rxxhslSRs2bNBTTz2lnj176rXXXtOMGTPUv39/eXt722c+S0tL5XQ6NXLkSN1555369NNPtXTpUnXr1k2S1LlzZ6Wmpto/59yLXubMmaOBAwfqueee0+rVq3Xw4EFdf/31ZV59fTbyXn/9dftnnzhxQkVFRXrllVdcfl9lfZ70uuuuk/Tfq7737t2rq6++WgEBAfY2e/fulSQ1adLEZd/vv/9et9xyi1q1aqVVq1bp1ltvVWBgoP37r1+/vsu/x9kz2eVd8AWg5iIuARhx4sQJBQcH66efftLdd99d7rZZWVm65ZZbFBgYqJdfflnbt29XvXr1tHHjRo0bN06enp6qU6eOy9viRUVFLrfeOXtBypQpU+Tn52cHT1FRkby9vbV8+XLdcsst9vaWZamoqEilpaWSpFdffVXffPONtm7dqvvvv1+SNHHiRElSdHS07rvvPjVo0MDev1evXvrLX/6i+Ph4bdu2TT4+Pho8eLD69OmjkJAQWZalG2+8Ub6+vsrMzNS6devst+BPnjypxMREXX755XrzzTf15ptv2sfdsGGDSktLFRUVpSlTpiguLk6bNm3SHXfcYV+x/cADD2j69Ok6ffq0br/99jJ/p2dvkfT000/bZwQXL16sQ4cOacSIEfZ2Y8aMcbmdU1ksy9K///3v864Iz8rKkiSXaJfOhOmUKVM0YMAA+fv7l3tsALUfb4sDuGRn7x25ceNGZWZm2heunA253woNDVV4eLg6dOigrl27aubMmVq5cqXq1aunrVu3KioqSr/88ou9/aJFi3TDDTe43GPy3XffVf369dWhQwe1bt1aPj4+cnd3ty/aufXWW13eIq5bt64CAgLs4IuLi9Ps2bPVqlUrl7F9+eWX+vHHHzVmzBiX9e7u7nr//fe1Z88elzOBISEhks5E3SOPPCLpzFnSIUOG2DE2b948TZkyxeVq97MCAwP18ccf29/gk56erokTJ2r48OH2xwtGjBihr7/+Wps3b9bDDz9c5u+0efPmmjlzpn0v0LP3+rQsy14uLi7W5MmTdf3115d5jLNmz56t3bt3n3c/y0OHDikkJOS81+Hm5qahQ4faYfl7H+W/0H8XAGoHzlwCqLSz90t88cUXNWXKFH3wwQfq1auXLrvsMkn/Pbt41tno8Pb2tr+r+vjx41q5cqU+/vhjvfTSS+rfv7+6deumRo0a2fvdeuuteuqppzR69Gi98847kmRfWX3WwoULFR8fr88++0yxsbH69NNP9dxzz6lVq1b2WcNzXXPNNbrmmmtcxlVcXKwnnnhCsbGx9gUv54ZSZGSk7rzzTh06dOi838XYsWPVvHlzpaam6tprr9W1116rpUuX6tFHH9WMGTOUkJDg8prONWjQIIWHh2vdunXq0qWL8vLy5Ofnpx9//FHt27dXYWGh/P39VVpaqtzc3DKPM3bsWCUnJ7uc3T127JiKior0xhtv2OuKiooUHR2tTz/9tMyxfP7553r88cf1+OOP2xclnfXzzz8rPDy8zP3Ode4fAb81ffp0+6s2Q0NDf/dYAGoe4hJApaWkpEiS7r77brVu3Vp/+9vf7GiUpIYNG2r06NH2clFRkYqKipSSkqIFCxYoOTlZP/74ozp06KDOnTurV69eioqK0ocffig3NzcVFBTI3d1dQUFBdrjedddd9tvdTqdTq1ev1owZM/TVV19p9uzZ6t27t/2zv/76az3wwAOKiIhQ//799de//lU333zzeWfezo7L09NTn3zyiXx9fbV582YtX75cq1ev1j333GNvu2jRIpfPPP7nP/+Ru7u7PDw89M4776ikpEQ7d+7UtGnTFBISohkzZmj//v3q2rWrUlNTVVxcrPDwcJebovv7++uGG27Q0qVLFRAQoPnz52vt2rVav369srKy9PDDD6t///7at2+funTpoqVLl6pDhw4ur+GNN96Qt7e3y9vSL774otLS0vTZZ5/Z64qLi8u8ifrhw4f1xhtvaPr06erfv7/LW/eJiYnau3evkpKSNHDgwHL+izjD3d3d5XOdZ8+gSmcuilq3bp3+/ve/n3fFO4Ba4k/9skkAtUpKSop13XXXWTk5OVZiYqL16KOPlrt9w4YNrX//+9/WggULrKZNm1oTJkywsrKyLMuyrGeffdbq0qWLlZ+fb/38889W7969rZCQEGv48OH2/vfdd5/9HeAzZ860/P39LUlW9+7drfT0dMuyLKu4uNiSZK1du9beb/HixVZUVJQlyYqMjLROnz7tMq7WrVtbb7/9tsu67du3W1deeaX10EMP2WMsS4sWLSwfHx8rICDgdx9+fn6Wl5eXlZiYaO+/ZMkS6+6777b8/PysoUOHWsePH7csy7L27t1r9e7d26pbt641ffp0y7IsKy8vz7rlllusOnXqWPPmzbNfr5+fnxUeHm5deeWVLo+wsDCrUaNG561v3ry5FRAQYKWmplqWdeZ7vvv27WuFhIRYSUlJ573G119/3WratKk1dOhQKzs7u5x/4bI9/fTTVrdu3SzLsqzS0tIK7w+gZuE+lwAuSWlpqf11j78nLy/PvlLb3d3dZb/i4mIVFhba96YcOXKkrrrqKj3wwANlXiRy4MABPfvssxoyZIjLhTtn7wX52WefnXdh0ebNm+Xp6Wnf3Ls6SEtL0+LFixUfH6+wsDB7fXFxsV5//XU98MADLldnl5aWavbs2Ro0aJDRq63z8vJUWFhof4bUpFGjRmnz5s0u32YEoPYiLgEAAGAMV4sDAADAGOISAAAAxhCXAAAAMKZa3IqotLRUhw8fVv369e2vLQMAAED1YVmWcnNz1bhx43Iv5KwWcXn48OGLujEvAAAAqlZmZuZ5XwN7rmoRl/Xr15d0ZrB8Ly0AAED1k5OTo/DwcLvbLqRaxOXZt8L9/f2JSwAAgGrs9z7CyAU9AAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxnhW9QBqo2bPLKvqIaAcGZN7V/UQcAHMneqLeQPgYnHmEgAAAMYQlwAAADCGuAQAAIAxxCUAAACMIS4BAABgDHEJAAAAY4hLAAAAGENcAgAAwBjiEgAAAMYQlwAAADCGuAQAAIAxxCUAAACMIS4BAABgDHEJAAAAY4hLAAAAGENcAgAAwBjiEgAAAMYQlwAAADCGuAQAAIAxxCUAAACMIS4BAABgDHEJAAAAY4hLAAAAGENcAgAAwBjiEgAAAMYQlwAAADCGuAQAAIAxxCUAAACMIS4BAABgDHEJAAAAY4hLAAAAGENcAgAAwBjiEgAAAMYQlwAAADCGuAQAAIAxxCUAAACMIS4BAABgDHEJAAAAY4hLAAAAGENcAgAAwBjiEgAAAMYQlwAAADCGuAQAAIAxxCUAAACMIS4BAABgDHEJAAAAY4hLAAAAGENcAgAAwBjiEgAAAMYQlwAAADCm0nHZs2dPzZkzR5KUlpamjh07KigoSAkJCbIsy9T4AAAAUINUKi7nz5+vVatWSZKcTqf69Omj6OhopaSkKD093Y5OAAAA/P9S4bg8ceKERo8erSuvvFKStGLFCjkcDk2dOlURERGaOHGiZs2aVe4xnE6ncnJyXB4AAACo+Socl6NHj9add96pmJgYSVJqaqpiYmLk6+srSYqMjFR6enq5x5g0aZICAgLsR3h4eCWGDgAAgOqmQnG5fv16rV27Vq+99pq9LicnR82bN7eX3dzc5OHhoezs7AseZ9y4cXI4HPYjMzOzEkMHAABAdeN5sRsWFhbqscce03vvvSd/f///HsDTU97e3i7b+vj4KD8/X0FBQWUey9vb+7x9AAAAUPNd9JnLCRMmqGPHjurdu7fL+uDgYB09etRlXW5urry8vMyMEAAAADXGRZ+5/Pjjj3X06FEFBgZKkvLz87Vo0SI1a9ZMp0+ftrfLyMiQ0+lUcHCw8cECAACgervouExOTlZxcbG9PGbMGMXExGjw4MFq06aN5s6dq0GDBmny5Mnq0aOHPDw8/pABAwAAoPq66Li84oorXJb9/PzUoEEDNWjQQElJSRowYIASEhJUUlKijRs3Gh8oAAAAqr+LjsvfOvdG6f369dPu3buVkpKiTp06KTQ01MTYAAAAUMNUOi5/KywsTGFhYaYOBwAAgBqo0t8tDgAAAPwWcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMMazqgcAAAAuTbNnllX1EHABGZN7V/UQ/nScuQQAAIAxxCUAAACMqVRcHj9+XN9++62OHTtmejwAAACowSoclwsXLlTLli01YsQINWnSRAsXLpQkpaWlqWPHjgoKClJCQoIsyzI+WAAAAFRvFYrLkydP6oknnlBycrJ+/PFHvf/++xo7dqycTqf69Omj6OhopaSkKD09XXPmzPmDhgwAAIDqqkJxmZubq2nTpqldu3aSpGuvvVbZ2dlasWKFHA6Hpk6dqoiICE2cOFGzZs264HGcTqdycnJcHgAAAKj5KhSX4eHhGjhwoCTp9OnTeuONN3TXXXcpNTVVMTEx8vX1lSRFRkYqPT39gseZNGmSAgIC7Ed4ePglvAQAAABUF5W6oCc1NVWNGjXS6tWrNW3aNOXk5Kh58+b2825ubvLw8FB2dnaZ+48bN04Oh8N+ZGZmVm70AAAAqFYqFZeRkZFau3at2rZtq/j4eHl6esrb29tlGx8fH+Xn55e5v7e3t/z9/V0eAAAAqPkqFZdubm5q37695syZoy+++ELBwcE6evSoyza5ubny8vIyMkgAAADUDBWKy3Xr1ikhIcFe9vQ88+2RV111lTZt2mSvz8jIkNPpVHBwsKFhAgAAoCaoUFxeddVVev/995WUlKTMzEw988wzuu2229S7d285HA7NnTtXkjR58mT16NFDHh4ef8igAQAAUD1VKC4bN26sTz/9VNOmTVPbtm2Vn5+vefPmydPTU0lJSRo6dKgaNWqkzz77TJMnT/6jxgwAAIBqyrOiO9x+++1l3maoX79+2r17t1JSUtSpUyeFhoYaGSAAAABqjgrHZXnCwsIUFhZm8pAAAACoQSp1tTgAAABQFuISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxlQoLr/44gu1aNFCnp6euv7667Vjxw5JUlpamjp27KigoCAlJCTIsqw/ZLAAAACo3i46Lvfu3av4+HhNnjxZhw4dUtOmTfXII4/I6XSqT58+io6OVkpKitLT0zVnzpw/cMgAAACori46Lnfs2KGJEyfq3nvvVaNGjTRs2DClpKRoxYoVcjgcmjp1qiIiIjRx4kTNmjWr3GM5nU7l5OS4PAAAAFDzeV7shrGxsS7Lu3btUsuWLZWamqqYmBj5+vpKkiIjI5Wenl7usSZNmqSXXnqpEsMFAABAdVapC3qKior0xhtvaPjw4crJyVHz5s3t59zc3OTh4aHs7OwL7j9u3Dg5HA77kZmZWZlhAAAAoJqpVFw+99xz8vPz06OPPipPT095e3u7PO/j46P8/PwL7u/t7S1/f3+XBwAAAGq+i35b/Kw1a9YoMTFRmzZtUp06dRQcHKy0tDSXbXJzc+Xl5WVskAAAAKgZKnTmct++fRo4cKDee+89tWnTRpLUsWNHbdq0yd4mIyNDTqdTwcHBZkcKAACAau+i47KgoECxsbHq16+f4uLidOrUKZ06dUqdO3eWw+HQ3LlzJUmTJ09Wjx495OHh8YcNGgAAANXTRb8tvmrVKu3YsUM7duzQzJkz7fX79+9XUlKSBgwYoISEBJWUlGjjxo1/yGABAABQvV10XPbr1++C37zTrFkz7d69WykpKerUqZNCQ0ONDRAAAAA1R4Uv6LmQsLAwhYWFmTocAAAAaqBK3YoIAAAAKAtxCQAAAGOISwAAABhDXAIAAMAY4hIAAADGEJcAAAAwhrgEAACAMcQlAAAAjCEuAQAAYAxxCQAAAGOISwAAABhDXAIAAMAY4hIAAADGEJcAAAAwhrgEAACAMcQlAAAAjCEuAQAAYAxxCQAAAGOISwAAABhDXAIAAMAY4hIAAADGEJcAAAAwhrgEAACAMcQlAAAAjCEuAQAAYAxxCQAAAGOISwAAABhDXAIAAMAY4hIAAADGEJcAAAAwhrgEAACAMcQlAAAAjCEuAQAAYAxxCQAAAGOISwAAABhDXAIAAMAY4hIAAADGEJcAAAAwhrgEAACAMcQlAAAAjCEuAQAAYAxxCQAAAGOISwAAABhDXAIAAMAY4hIAAADGEJcAAAAwhrgEAACAMcQlAAAAjCEuAQAAYAxxCQAAAGOISwAAABhDXAIAAMAY4hIAAADGEJcAAAAwhrgEAACAMcQlAAAAjCEuAQAAYAxxCQAAAGOISwAAABhDXAIAAMAY4hIAAADGEJcAAAAwhrgEAACAMcQlAAAAjCEuAQAAYAxxCQAAAGMqHJfHjx9X8+bNlZGRYa9LS0tTx44dFRQUpISEBFmWZXKMAAAAqCEqFJfHjh1TbGysS1g6nU716dNH0dHRSklJUXp6uubMmWN4mAAAAKgJKhSX/fv3V//+/V3WrVixQg6HQ1OnTlVERIQmTpyoWbNmlXscp9OpnJwclwcAAABqvgrFZVJSkp588kmXdampqYqJiZGvr68kKTIyUunp6eUeZ9KkSQoICLAf4eHhFRw2AAAAqqMKxWWLFi3OW5eTk6PmzZvby25ubvLw8FB2dvYFjzNu3Dg5HA77kZmZWZFhAAAAoJryvOQDeHrK29vbZZ2Pj4/y8/MVFBRU5j7e3t7n7QMAAICa75JvRRQcHKyjR4+6rMvNzZWXl9elHhoAAAA1zCXHZceOHbVp0yZ7OSMjQ06nU8HBwZd6aAAAANQwlxyXXbp0kcPh0Ny5cyVJkydPVo8ePeTh4XHJgwMAAEDNYuQzl0lJSRowYIASEhJUUlKijRs3mhgbAAAAaphKxeVvv4GnX79+2r17t1JSUtSpUyeFhoYaGRwAAABqlks+c3lWWFiYwsLCTB0OAAAANdAlf+YSAAAAOIu4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHEJQAAAIwhLgEAAGAMcQkAAABjiEsAAAAYQ1wCAADAGOISAAAAxhCXAAAAMIa4BAAAgDHG4jItLU0dO3ZUUFCQEhISZFmWqUMDAACghjASl06nU3369FF0dLRSUlKUnp6uOXPmmDg0AAAAahBPEwdZsWKFHA6Hpk6dKl9fX02cOFEjRoxQfHx8mds7nU45nU572eFwSJJycnJMDKfKlTrzq3oIKEdt+e+sNmLuVF/Mm+qNuVN91aa5c/a1/N67026WgfevX3rpJW3evFnLly+3f2hISIhOnDhR5vYvvviiXnrppUv9sQAAAPiTZWZm6oorrrjg80bOXObk5Kh58+b2spubmzw8PJSdna2goKDzth83bpyeeuope7m0tFQnTpxQSEiI3NzcTAwJhuTk5Cg8PFyZmZny9/ev6uEANQZzB6g45k31ZlmWcnNz1bhx43K3MxKXnp6e8vb2dlnn4+Oj/Pz8MuPS29v7vO0DAwNNDAV/EH9/fyY6UAnMHaDimDfVV0BAwO9uY+SCnuDgYB09etRlXW5urry8vEwcHgAAADWEkbjs2LGjNm3aZC9nZGTI6XQqODjYxOEBAABQQxiJyy5dusjhcGju3LmSpMmTJ6tHjx7y8PAwcXhUIW9vb40fP/68jzEAKB9zB6g45k3tYORqcUlavHixBgwYoPr166ukpEQbN25U27ZtTRwaAAAANYSxuJSkQ4cOKSUlRZ06dVJoaKipwwIAAKCGMBqXAAAA+P/N2HeLAwAAAMQlAAAAjCEuAQAAYAxxCQAAAGOISwD4A+Tk5Cg7O7uqhwEAfzriEpKk5ORkdevWTVFRUZowYYJKSkr01FNPKTg4WKGhoRoxYoQKCwurephAtZOdna0hQ4bopptu0ssvvyyn06l77rlHgYGBatCggTp37qz9+/dX9TAB4E9DXEKnTp1S37591b17d02YMEHJycnq0KGDNm7cqCVLlmjJkiXas2ePRo0aVdVDBaqdRx99VEeOHNGwYcO0YcMG3XzzzQoICJDD4VB6erry8vL0yCOPVPUwAeBPw30uoe+++07Dhg3TTz/9JOlMbDZu3FgrV65Up06dJJ35vvjo6GgdP368CkcKVD+BgYHaunWrWrRooSNHjujyyy/X8ePHFRgYKEn66KOPNHToUBUUFFTtQIFqJiQkRCdPnrzg85Zlyc3NTSUlJX/eoGCEZ1UPAFWvWbNmOnjwoI4cOaKGDRvKz89Pn3zyiWJiYuxtUlNTFRAQUIWjBKonHx8flZaWSjrzh5llWTp27Jgdl4WFhWrQoEEVjhConjZv3qzY2Fj9z//8j+65556qHg4MIi6hyy+/XM8++6yioqI0c+ZM9e7dW7169bKfnzp1ql588UUlJiZW4SiB6mn06NG66667dMcdd+jzzz9Xz5491a9fPw0bNky//vqr3n//fd1///1VPUyg2mnZsqVWrFih2NhY3X333WrWrFlVDwmG8LY4bLt371ZxcbGuvvpql/ULFizQVVddpfbt21fRyIDqLTk5WVu2bFG7du10++23a9GiRVq4cKGKi4t14403atSoUfL29q7qYQLAn4K4BAAAgDFcLQ4AAP50v/76qx588EFdffXV6tOnj1auXOnyfF5enjw8PKpodLgUxCUAAPjTPfTQQzp48KDGjh2rdu3aacCAAerbt6+OHTtmb8ObqzUTb4tDEreEACqLuQNUjre3t/bv36/GjRtLko4fP67hw4fr66+/1ieffKL27dvL39+fuVMDcbU4JHFLCKCymDtA5YSEhGjXrl12XIaEhOiTTz7Rhx9+qNjYWI0dO7aKR4jK4swlbPv371dsbKyWLVvGLSGACmDuABWXlJSk8ePH67333lO/fv1cntuyZYv69u2ro0ePcuayBiIuAQBAlfjmm2+0a9cuDRky5Lznjhw5ogULFujJJ5+sgpHhUhCXkCQdPHhQTZo0qephADUOcweoHOZO7UVcQpLk4eGh7Oxs+fv7V/VQgBqFuQNUDnOn9uJWRJD03ytaAVQMcweoHOZO7cWZS0iS3N3d1aRJE7m7l//3xr59+/6kEQE1A3MHqBzmTu3FrYhgmzhxourVq1fVwwBqHOYOUDnMndqJM5eQdOYvyJMnT/LZF6CCmDtA5TB3ai8+cwlJ0vjx4+Xj41PVwwBqHOYOUDnMndqLM5eosHvvvVfvvvuuQkNDq3ooQI3C3AEqh7lTs3DmEhW2Zs0aFRQUVPUwgBqHuQNUDnOnZiEuAQAAYAxxCQAAAGOISwAAABhDXKLC+EYFoHKYO0DlMHdqFuISFcYNBoDKYe4AlcPcqVmIS7jYsWOHdu3aZS+vWrVKO3bscNlm27ZtuuKKK/7soQHVGnMHqBzmTu1DXELSme9u7dChgzp06KA5c+bY64cMGaLIyEi1bdvW/n7X8PDw3/0uWOD/C+YOUDnMndqLm6hDktStWzd16tRJL7zwgry9vV2ey8vLU0JCgnbu3Kl169ZV0QiB6om5A1QOc6f2Ii4hSfLz81NaWpqaNWtW5vMHDhxQ27ZtderUqT93YEA1x9wBKoe5U3txjhmSpJiYGL3yyisqKioq8/m33npLUVFRf+6ggBqAuQNUDnOn9uLMJSRJWVlZ6tevn/bv36/o6Gg1atRIHh4eys7O1tatW+Xm5qZVq1bp6quvruqhAtUKcweoHOZO7UVcwsX69ev17bff6tChQzp9+rQCAwPVvn17xcXFqV69elU9PKDaYu4AlcPcqX08q3oAqF5atmypoqIiRURESJKCg4PVrl07JjjwO5g7QOUwd2of4hKSpIMHD2rgwIH69ttv5efnp4CAAFmWJYfDoby8PHXt2lVz585VWFhYVQ8VqFaYO0DlMHdqLy7ogSQpPj5ejRo1UkZGhhwOhw4ePKjMzEzl5ORoz5498vPz00MPPVTVwwSqHeYOUDnMndqLz1xCklS3bl3t2LHjgreE2Ldvn9q2bauCgoI/d2BANcfcASqHuVN7ceYSkqRWrVrpo48+uuDzH374oVq1avUnjgioGZg7QOUwd2ovzlxCkvT1118rLi5OAQEBioyMVEBAgCQpOztbP/30kwoLC7VkyRLFxMRU8UiB6oW5A1QOc6f2Ii5hczgcWrJkidLS0pSdnS1JCgkJUZs2bRQQEKDvvvtOkyZNquJRAtUPcweoHOZO7URcokxpaWlatWqVvvzySyUnJ6u4uFgxMTHasGFDVQ8NqNaYO0DlMHdqD25FBEnSL7/8ojVr1tiPkydPKioqSj/++KM++OAD3XnnndxzDCgDcweoHOZO7cWZS0iS3N3d5ebmpq5du2r06NHq0aOHvLy8FBQUpNTUVDVp0qSqhwhUS8wdoHKYO7UXZy4hSS5/PcbFxal169aKiYmR0+nUkSNHmOTABTB3gMph7tRenLnEeY4dO+Yy6Q8fPqyWLVuqe/fumjFjRlUPD6i2mDtA5TB3ahfiEr9rx44d9oesly5dWtXDAWoM5g5QOcydmo24BAAAgDF8Qw8AAACMIS4BAABgDHEJAAAAY4hLAAAAGENcAgAAwBjiEgAAAMYQlwAAADDm/wBn29owaDkr1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 模型参数 -> mlp_parameter_data/mlp_weighted_bce_opt_thresh.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akuta\\AppData\\Local\\Temp\\ipykernel_15352\\1962234394.py:143: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pos_weights = torch.tensor((neg_counts / (pos_counts + 1e-6)) * 0.5, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 1.6670\n",
      "Epoch 20/200, Loss: 1.6755\n",
      "Epoch 30/200, Loss: 1.6887\n",
      "Epoch 40/200, Loss: 1.6795\n",
      "Epoch 50/200, Loss: 1.6737\n",
      "Epoch 60/200, Loss: 1.6745\n",
      "Epoch 70/200, Loss: 1.6726\n",
      "Epoch 80/200, Loss: 1.6717\n",
      "Epoch 90/200, Loss: 1.6705\n",
      "Epoch 100/200, Loss: 1.6739\n",
      "Epoch 110/200, Loss: 1.6819\n",
      "Epoch 120/200, Loss: 1.6758\n",
      "Epoch 130/200, Loss: 1.6609\n",
      "Epoch 140/200, Loss: 1.6669\n",
      "Epoch 150/200, Loss: 1.6892\n",
      "Epoch 160/200, Loss: 1.6818\n",
      "Epoch 170/200, Loss: 1.6757\n",
      "Epoch 180/200, Loss: 1.6599\n",
      "Epoch 190/200, Loss: 1.6799\n",
      "Epoch 200/200, Loss: 1.6831\n",
      "Best thresholds per label: [0.5 0.5 0.5]\n",
      "\n",
      "=== 无过采样MLP + Weighted BCE + Optimized Threshold 模型评估指标 ===\n",
      "Hamming Loss: 0.4848\n",
      "Subset Accuracy: 0.0413\n",
      "Precision (micro): 0.0757\n",
      "Recall (micro): 0.7368\n",
      "F1-score (micro): 0.1373\n",
      "Precision (macro): 0.0871\n",
      "Recall (macro): 0.6389\n",
      "F1-score (macro): 0.1473\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 模型参数 -> mlp_parameter_data/nomlp_weighted_bce_opt_thresh.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------ MLP 模型 ------------------\n",
    "class MultiLabelMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultiLabelMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ------------------ 自动搜索标签阈值 ------------------\n",
    "def search_best_threshold(model, val_loader, y_val, thresholds=np.arange(0.1, 0.9, 0.05)):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in val_loader:\n",
    "            logits = model(batch_x)\n",
    "            all_logits.append(logits)\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    \n",
    "    best_thresholds = []\n",
    "    for i in range(y_val.shape[1]):\n",
    "        best_f1 = 0\n",
    "        best_t = 0.5\n",
    "        for t in thresholds:\n",
    "            pred_i = (all_logits[:, i] >= t).astype(int)\n",
    "            f1 = f1_score(y_val[:, i], pred_i, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_t = t\n",
    "        best_thresholds.append(best_t)\n",
    "    return np.array(best_thresholds)\n",
    "\n",
    "# ------------------ 主流程 ------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 转换为 PyTorch Tensor\n",
    "    X_val_selected = X_val[selected_feats].values\n",
    "    train_dataset = TensorDataset(torch.tensor(train_x_res_scaled, dtype=torch.float32),\n",
    "                                  torch.tensor(train_y_res.values, dtype=torch.float32))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val_selected, dtype=torch.float32),\n",
    "                                torch.tensor(y_val.values, dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_x_selected, dtype=torch.float32),\n",
    "                                 torch.tensor(test_y.values, dtype=torch.float32))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # MLP 初始化\n",
    "    input_dim = len(selected_feats)\n",
    "    hidden_dim = 64\n",
    "    output_dim = train_y_res.shape[1]\n",
    "    \n",
    "    model = MultiLabelMLP(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    # 计算标签加权（微调比例避免过度预测）\n",
    "    n_samples = train_y_res.shape[0]\n",
    "    pos_counts = train_y_res.sum(axis=0)\n",
    "    neg_counts = n_samples - pos_counts\n",
    "    pos_weights = torch.tensor((neg_counts / (pos_counts + 1e-6)) * 0.5, dtype=torch.float32)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # --------- 训练 ----------\n",
    "    epochs = 200\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "        epoch_loss /= n_samples\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # --------- 搜索每个标签最优阈值 ----------\n",
    "    best_thresholds = search_best_threshold(model, val_loader, y_val.values)\n",
    "    print(\"Best thresholds per label:\", best_thresholds)\n",
    "    \n",
    "    # --------- 测试预测 ----------\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in test_loader:\n",
    "            logits = model(batch_x)\n",
    "            preds = (torch.sigmoid(logits).numpy() >= best_thresholds).astype(int)\n",
    "            all_preds.append(preds)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    \n",
    "    # --------- 评估指标 ----------\n",
    "    metrics = evaluate_multilabel(test_y.values, all_preds)\n",
    "    print(\"\\n=== MLP + Weighted BCE + Optimized Threshold 模型评估指标 ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus']=False \n",
    "    plt.figure(figsize=(8,5))\n",
    "    pd.DataFrame(train_y_res, columns=label_cols).sum(axis=0).plot(kind='bar', title='训练集标签数量分布')\n",
    "    plt.show()\n",
    "    \n",
    "    # --------- 保存模型 ----------\n",
    "    os.makedirs('mlp_parameter_data', exist_ok=True)\n",
    "    torch.save(model.state_dict(), 'mlp_parameter_data/mlp_weighted_bce_opt_thresh.pth')\n",
    "    print(\"[saved] 模型参数 -> mlp_parameter_data/mlp_weighted_bce_opt_thresh.pth\")\n",
    "\n",
    "\n",
    "# #无过采样\n",
    "    train_dataset1 = TensorDataset(torch.tensor(train_x_selected, dtype=torch.float32),\n",
    "                                  torch.tensor(train_y.values, dtype=torch.float32))\n",
    "    val_dataset1 = TensorDataset(torch.tensor(X_val_selected, dtype=torch.float32),\n",
    "                                torch.tensor(y_val.values, dtype=torch.float32))\n",
    "    test_dataset1 = TensorDataset(torch.tensor(test_x_selected, dtype=torch.float32),\n",
    "                                 torch.tensor(test_y.values, dtype=torch.float32))\n",
    "    \n",
    "    train_loader1 = DataLoader(train_dataset1, batch_size=32, shuffle=True)\n",
    "    val_loader1 = DataLoader(val_dataset1, batch_size=32, shuffle=False)\n",
    "    test_loader1 = DataLoader(test_dataset1, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # MLP 初始化\n",
    "    input_dim1 = len(selected_feats)\n",
    "    hidden_dim1 = 64\n",
    "    output_dim1 = train_y.shape[1]\n",
    "    \n",
    "    model1 = MultiLabelMLP(input_dim1, hidden_dim1, output_dim1)\n",
    "    \n",
    "    # 计算标签加权（微调比例避免过度预测）\n",
    "    n_samples = train_y.shape[0]\n",
    "    pos_counts = train_y.sum(axis=0)\n",
    "    neg_counts = n_samples - pos_counts\n",
    "    pos_weights = torch.tensor((neg_counts / (pos_counts + 1e-6)) * 0.5, dtype=torch.float32)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # --------- 训练 ----------\n",
    "    epochs = 200\n",
    "    model1.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model1(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "        epoch_loss /= n_samples\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # --------- 搜索每个标签最优阈值 ----------\n",
    "    best_thresholds1 = search_best_threshold(model1, val_loader, y_val.values)\n",
    "    print(\"Best thresholds per label:\", best_thresholds1)\n",
    "    \n",
    "    # --------- 测试预测 ----------\n",
    "    model1.eval()\n",
    "    all_preds1 = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in test_loader:\n",
    "            logits = model1(batch_x)\n",
    "            preds = (torch.sigmoid(logits).numpy() >= best_thresholds).astype(int)\n",
    "            all_preds1.append(preds)\n",
    "    all_preds1 = np.vstack(all_preds1)\n",
    "    \n",
    "    # --------- 评估指标 ----------\n",
    "    metrics1 = evaluate_multilabel(test_y.values, all_preds1)\n",
    "    print(\"\\n=== 无过采样MLP + Weighted BCE + Optimized Threshold 模型评估指标 ===\")\n",
    "    for k, v in metrics1.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    # --------- 标签分布可视化 ----------\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus']=False \n",
    "    plt.figure(figsize=(8,5))\n",
    "    pd.DataFrame(train_y, columns=label_cols).sum(axis=0).plot(kind='bar', title='训练集标签数量分布')\n",
    "    plt.show()\n",
    "    \n",
    "    # --------- 保存模型 ----------\n",
    "    os.makedirs('mlp_parameter_data', exist_ok=True)\n",
    "    torch.save(model.state_dict(), 'mlp_parameter_data/nomlp_weighted_bce_opt_thresh.pth')\n",
    "    print(\"[saved] 模型参数 -> mlp_parameter_data/nomlp_weighted_bce_opt_thresh.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfa3a03f-7619-4473-b2be-6f1b9470317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "最佳参数：\n",
      "  estimator__l2_regularization: 0.01563640674119393\n",
      "  estimator__learning_rate: 0.09468029614127392\n",
      "  estimator__max_bins: 128\n",
      "  estimator__max_depth: 6\n",
      "  estimator__max_iter: 235\n",
      "MLHGB (AutoTune) Hamming Loss=0.0689\n",
      "\n",
      "===梯度boosting评估指标 ===\n",
      "Hamming Loss: 0.0689\n",
      "Subset Accuracy: 0.8347\n",
      "Precision (micro): 0.2500\n",
      "Recall (micro): 0.1579\n",
      "F1-score (micro): 0.1935\n",
      "Precision (macro): 0.1429\n",
      "Recall (macro): 0.0833\n",
      "F1-score (macro): 0.1053\n",
      "[saved] MLHGB 自动调参实验结果 -> mlhgb_autotune_data\\mlhgb_autotune_experiment.npz\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "最佳参数：\n",
      "  estimator__l2_regularization: 0.5426960831582485\n",
      "  estimator__learning_rate: 0.03818484499495253\n",
      "  estimator__max_bins: 142\n",
      "  estimator__max_depth: 7\n",
      "  estimator__max_iter: 164\n",
      "MLHGB (AutoTune) Hamming Loss=0.0523\n",
      "\n",
      "===梯度boosting评估指标 ===\n",
      "Hamming Loss: 0.0523\n",
      "Subset Accuracy: 0.8678\n",
      "Precision (micro): 0.5000\n",
      "Recall (micro): 0.1579\n",
      "F1-score (micro): 0.2400\n",
      "Precision (macro): 0.2000\n",
      "Recall (macro): 0.0833\n",
      "F1-score (macro): 0.1176\n",
      "[saved] 无过采样MLHGB 自动调参实验结果 -> nomlhgb_autotune_data\\nomlhgb_autotune_experiment.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "\n",
    "# ====== 多标签直方图梯度提升 + 自动调参 ======\n",
    "def run_ml_histgb_autotune(train_x, train_y, test_x, test_y, n_iter=20):\n",
    "\n",
    "    base_hgb = HistGradientBoostingClassifier(\n",
    "        early_stopping=False,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 包装成多输出分类器\n",
    "    multi_hgb = MultiOutputClassifier(base_hgb, n_jobs=-1)\n",
    "\n",
    "    # 定义搜索空间（针对内部的 base_estimator 参数）\n",
    "    param_distributions = {\n",
    "        \"estimator__learning_rate\": uniform(0.01, 0.2),  # [0.01, 0.21)\n",
    "        \"estimator__max_iter\": randint(100, 600),\n",
    "        \"estimator__max_depth\": randint(3, 10),\n",
    "        \"estimator__l2_regularization\": uniform(0.0, 1.0),\n",
    "        \"estimator__max_bins\": randint(128, 255),\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        multi_hgb,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,              # 随机采样次数\n",
    "        scoring=\"f1_micro\",         # 使用 micro F1 评价整体性能\n",
    "        cv=3,                       # 3 折交叉验证\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 开始搜索\n",
    "    search.fit(train_x, train_y)\n",
    "\n",
    "    print(\"最佳参数：\")\n",
    "    for k, v in search.best_params_.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # 在测试集上预测\n",
    "    y_pred = best_model.predict(test_x)\n",
    "\n",
    "    # Hamming Loss\n",
    "    loss = hamming_loss(test_y.values, y_pred)\n",
    "    print(f\"MLHGB (AutoTune) Hamming Loss={loss:.4f}\")\n",
    "\n",
    "    # 评估指标\n",
    "    metrics = evaluate_multilabel(test_y.values, y_pred)\n",
    "    print(\"\\n===梯度boosting评估指标 ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    return best_model, y_pred, search.best_params_\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ====== 含过采样数据实验 ======\n",
    "    model, best_predict, best_params = run_ml_histgb_autotune(train_x_res_scaled, train_y_res, test_x_selected, test_y)\n",
    "\n",
    "    os.makedirs('mlhgb_autotune_data', exist_ok=True)\n",
    "    experiment_filepath = os.path.join('mlhgb_autotune_data', 'mlhgb_autotune_experiment.npz')\n",
    "\n",
    "    np.savez_compressed(\n",
    "        experiment_filepath,\n",
    "        predict=best_predict,\n",
    "        y_true=test_y.values,\n",
    "        best_params=np.array(list(best_params.items()), dtype=object),\n",
    "        hamming_loss=np.array([hamming_loss(test_y.values, best_predict)])\n",
    "    )\n",
    "    print(f\"[saved] MLHGB 自动调参实验结果 -> {experiment_filepath}\")\n",
    "\n",
    "\n",
    "    # ====== 无过采样数据实验 ======\n",
    "    model1, best_predict1, best_params1 = run_ml_histgb_autotune(train_x_selected, train_y, test_x_selected, test_y)\n",
    "\n",
    "    os.makedirs('nomlhgb_autotune_data', exist_ok=True)\n",
    "    experiment_filepath1 = os.path.join('nomlhgb_autotune_data', 'nomlhgb_autotune_experiment.npz')\n",
    "\n",
    "    np.savez_compressed(\n",
    "        experiment_filepath1,\n",
    "        predict=best_predict1,\n",
    "        y_true=test_y.values,\n",
    "        best_params=np.array(list(best_params1.items()), dtype=object),\n",
    "        hamming_loss=np.array([hamming_loss(test_y.values, best_predict1)])\n",
    "    )\n",
    "    print(f\"[saved] 无过采样MLHGB 自动调参实验结果 -> {experiment_filepath1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d8793-6a33-4644-9070-7a0a76f192be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
